<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `src/network.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>network.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">   1</span>
<span id="2">   2</span>
<span id="3">   3</span>
<span id="4">   4</span>
<span id="5">   5</span>
<span id="6">   6</span>
<span id="7">   7</span>
<span id="8">   8</span>
<span id="9">   9</span>
<span id="10">  10</span>
<span id="11">  11</span>
<span id="12">  12</span>
<span id="13">  13</span>
<span id="14">  14</span>
<span id="15">  15</span>
<span id="16">  16</span>
<span id="17">  17</span>
<span id="18">  18</span>
<span id="19">  19</span>
<span id="20">  20</span>
<span id="21">  21</span>
<span id="22">  22</span>
<span id="23">  23</span>
<span id="24">  24</span>
<span id="25">  25</span>
<span id="26">  26</span>
<span id="27">  27</span>
<span id="28">  28</span>
<span id="29">  29</span>
<span id="30">  30</span>
<span id="31">  31</span>
<span id="32">  32</span>
<span id="33">  33</span>
<span id="34">  34</span>
<span id="35">  35</span>
<span id="36">  36</span>
<span id="37">  37</span>
<span id="38">  38</span>
<span id="39">  39</span>
<span id="40">  40</span>
<span id="41">  41</span>
<span id="42">  42</span>
<span id="43">  43</span>
<span id="44">  44</span>
<span id="45">  45</span>
<span id="46">  46</span>
<span id="47">  47</span>
<span id="48">  48</span>
<span id="49">  49</span>
<span id="50">  50</span>
<span id="51">  51</span>
<span id="52">  52</span>
<span id="53">  53</span>
<span id="54">  54</span>
<span id="55">  55</span>
<span id="56">  56</span>
<span id="57">  57</span>
<span id="58">  58</span>
<span id="59">  59</span>
<span id="60">  60</span>
<span id="61">  61</span>
<span id="62">  62</span>
<span id="63">  63</span>
<span id="64">  64</span>
<span id="65">  65</span>
<span id="66">  66</span>
<span id="67">  67</span>
<span id="68">  68</span>
<span id="69">  69</span>
<span id="70">  70</span>
<span id="71">  71</span>
<span id="72">  72</span>
<span id="73">  73</span>
<span id="74">  74</span>
<span id="75">  75</span>
<span id="76">  76</span>
<span id="77">  77</span>
<span id="78">  78</span>
<span id="79">  79</span>
<span id="80">  80</span>
<span id="81">  81</span>
<span id="82">  82</span>
<span id="83">  83</span>
<span id="84">  84</span>
<span id="85">  85</span>
<span id="86">  86</span>
<span id="87">  87</span>
<span id="88">  88</span>
<span id="89">  89</span>
<span id="90">  90</span>
<span id="91">  91</span>
<span id="92">  92</span>
<span id="93">  93</span>
<span id="94">  94</span>
<span id="95">  95</span>
<span id="96">  96</span>
<span id="97">  97</span>
<span id="98">  98</span>
<span id="99">  99</span>
<span id="100"> 100</span>
<span id="101"> 101</span>
<span id="102"> 102</span>
<span id="103"> 103</span>
<span id="104"> 104</span>
<span id="105"> 105</span>
<span id="106"> 106</span>
<span id="107"> 107</span>
<span id="108"> 108</span>
<span id="109"> 109</span>
<span id="110"> 110</span>
<span id="111"> 111</span>
<span id="112"> 112</span>
<span id="113"> 113</span>
<span id="114"> 114</span>
<span id="115"> 115</span>
<span id="116"> 116</span>
<span id="117"> 117</span>
<span id="118"> 118</span>
<span id="119"> 119</span>
<span id="120"> 120</span>
<span id="121"> 121</span>
<span id="122"> 122</span>
<span id="123"> 123</span>
<span id="124"> 124</span>
<span id="125"> 125</span>
<span id="126"> 126</span>
<span id="127"> 127</span>
<span id="128"> 128</span>
<span id="129"> 129</span>
<span id="130"> 130</span>
<span id="131"> 131</span>
<span id="132"> 132</span>
<span id="133"> 133</span>
<span id="134"> 134</span>
<span id="135"> 135</span>
<span id="136"> 136</span>
<span id="137"> 137</span>
<span id="138"> 138</span>
<span id="139"> 139</span>
<span id="140"> 140</span>
<span id="141"> 141</span>
<span id="142"> 142</span>
<span id="143"> 143</span>
<span id="144"> 144</span>
<span id="145"> 145</span>
<span id="146"> 146</span>
<span id="147"> 147</span>
<span id="148"> 148</span>
<span id="149"> 149</span>
<span id="150"> 150</span>
<span id="151"> 151</span>
<span id="152"> 152</span>
<span id="153"> 153</span>
<span id="154"> 154</span>
<span id="155"> 155</span>
<span id="156"> 156</span>
<span id="157"> 157</span>
<span id="158"> 158</span>
<span id="159"> 159</span>
<span id="160"> 160</span>
<span id="161"> 161</span>
<span id="162"> 162</span>
<span id="163"> 163</span>
<span id="164"> 164</span>
<span id="165"> 165</span>
<span id="166"> 166</span>
<span id="167"> 167</span>
<span id="168"> 168</span>
<span id="169"> 169</span>
<span id="170"> 170</span>
<span id="171"> 171</span>
<span id="172"> 172</span>
<span id="173"> 173</span>
<span id="174"> 174</span>
<span id="175"> 175</span>
<span id="176"> 176</span>
<span id="177"> 177</span>
<span id="178"> 178</span>
<span id="179"> 179</span>
<span id="180"> 180</span>
<span id="181"> 181</span>
<span id="182"> 182</span>
<span id="183"> 183</span>
<span id="184"> 184</span>
<span id="185"> 185</span>
<span id="186"> 186</span>
<span id="187"> 187</span>
<span id="188"> 188</span>
<span id="189"> 189</span>
<span id="190"> 190</span>
<span id="191"> 191</span>
<span id="192"> 192</span>
<span id="193"> 193</span>
<span id="194"> 194</span>
<span id="195"> 195</span>
<span id="196"> 196</span>
<span id="197"> 197</span>
<span id="198"> 198</span>
<span id="199"> 199</span>
<span id="200"> 200</span>
<span id="201"> 201</span>
<span id="202"> 202</span>
<span id="203"> 203</span>
<span id="204"> 204</span>
<span id="205"> 205</span>
<span id="206"> 206</span>
<span id="207"> 207</span>
<span id="208"> 208</span>
<span id="209"> 209</span>
<span id="210"> 210</span>
<span id="211"> 211</span>
<span id="212"> 212</span>
<span id="213"> 213</span>
<span id="214"> 214</span>
<span id="215"> 215</span>
<span id="216"> 216</span>
<span id="217"> 217</span>
<span id="218"> 218</span>
<span id="219"> 219</span>
<span id="220"> 220</span>
<span id="221"> 221</span>
<span id="222"> 222</span>
<span id="223"> 223</span>
<span id="224"> 224</span>
<span id="225"> 225</span>
<span id="226"> 226</span>
<span id="227"> 227</span>
<span id="228"> 228</span>
<span id="229"> 229</span>
<span id="230"> 230</span>
<span id="231"> 231</span>
<span id="232"> 232</span>
<span id="233"> 233</span>
<span id="234"> 234</span>
<span id="235"> 235</span>
<span id="236"> 236</span>
<span id="237"> 237</span>
<span id="238"> 238</span>
<span id="239"> 239</span>
<span id="240"> 240</span>
<span id="241"> 241</span>
<span id="242"> 242</span>
<span id="243"> 243</span>
<span id="244"> 244</span>
<span id="245"> 245</span>
<span id="246"> 246</span>
<span id="247"> 247</span>
<span id="248"> 248</span>
<span id="249"> 249</span>
<span id="250"> 250</span>
<span id="251"> 251</span>
<span id="252"> 252</span>
<span id="253"> 253</span>
<span id="254"> 254</span>
<span id="255"> 255</span>
<span id="256"> 256</span>
<span id="257"> 257</span>
<span id="258"> 258</span>
<span id="259"> 259</span>
<span id="260"> 260</span>
<span id="261"> 261</span>
<span id="262"> 262</span>
<span id="263"> 263</span>
<span id="264"> 264</span>
<span id="265"> 265</span>
<span id="266"> 266</span>
<span id="267"> 267</span>
<span id="268"> 268</span>
<span id="269"> 269</span>
<span id="270"> 270</span>
<span id="271"> 271</span>
<span id="272"> 272</span>
<span id="273"> 273</span>
<span id="274"> 274</span>
<span id="275"> 275</span>
<span id="276"> 276</span>
<span id="277"> 277</span>
<span id="278"> 278</span>
<span id="279"> 279</span>
<span id="280"> 280</span>
<span id="281"> 281</span>
<span id="282"> 282</span>
<span id="283"> 283</span>
<span id="284"> 284</span>
<span id="285"> 285</span>
<span id="286"> 286</span>
<span id="287"> 287</span>
<span id="288"> 288</span>
<span id="289"> 289</span>
<span id="290"> 290</span>
<span id="291"> 291</span>
<span id="292"> 292</span>
<span id="293"> 293</span>
<span id="294"> 294</span>
<span id="295"> 295</span>
<span id="296"> 296</span>
<span id="297"> 297</span>
<span id="298"> 298</span>
<span id="299"> 299</span>
<span id="300"> 300</span>
<span id="301"> 301</span>
<span id="302"> 302</span>
<span id="303"> 303</span>
<span id="304"> 304</span>
<span id="305"> 305</span>
<span id="306"> 306</span>
<span id="307"> 307</span>
<span id="308"> 308</span>
<span id="309"> 309</span>
<span id="310"> 310</span>
<span id="311"> 311</span>
<span id="312"> 312</span>
<span id="313"> 313</span>
<span id="314"> 314</span>
<span id="315"> 315</span>
<span id="316"> 316</span>
<span id="317"> 317</span>
<span id="318"> 318</span>
<span id="319"> 319</span>
<span id="320"> 320</span>
<span id="321"> 321</span>
<span id="322"> 322</span>
<span id="323"> 323</span>
<span id="324"> 324</span>
<span id="325"> 325</span>
<span id="326"> 326</span>
<span id="327"> 327</span>
<span id="328"> 328</span>
<span id="329"> 329</span>
<span id="330"> 330</span>
<span id="331"> 331</span>
<span id="332"> 332</span>
<span id="333"> 333</span>
<span id="334"> 334</span>
<span id="335"> 335</span>
<span id="336"> 336</span>
<span id="337"> 337</span>
<span id="338"> 338</span>
<span id="339"> 339</span>
<span id="340"> 340</span>
<span id="341"> 341</span>
<span id="342"> 342</span>
<span id="343"> 343</span>
<span id="344"> 344</span>
<span id="345"> 345</span>
<span id="346"> 346</span>
<span id="347"> 347</span>
<span id="348"> 348</span>
<span id="349"> 349</span>
<span id="350"> 350</span>
<span id="351"> 351</span>
<span id="352"> 352</span>
<span id="353"> 353</span>
<span id="354"> 354</span>
<span id="355"> 355</span>
<span id="356"> 356</span>
<span id="357"> 357</span>
<span id="358"> 358</span>
<span id="359"> 359</span>
<span id="360"> 360</span>
<span id="361"> 361</span>
<span id="362"> 362</span>
<span id="363"> 363</span>
<span id="364"> 364</span>
<span id="365"> 365</span>
<span id="366"> 366</span>
<span id="367"> 367</span>
<span id="368"> 368</span>
<span id="369"> 369</span>
<span id="370"> 370</span>
<span id="371"> 371</span>
<span id="372"> 372</span>
<span id="373"> 373</span>
<span id="374"> 374</span>
<span id="375"> 375</span>
<span id="376"> 376</span>
<span id="377"> 377</span>
<span id="378"> 378</span>
<span id="379"> 379</span>
<span id="380"> 380</span>
<span id="381"> 381</span>
<span id="382"> 382</span>
<span id="383"> 383</span>
<span id="384"> 384</span>
<span id="385"> 385</span>
<span id="386"> 386</span>
<span id="387"> 387</span>
<span id="388"> 388</span>
<span id="389"> 389</span>
<span id="390"> 390</span>
<span id="391"> 391</span>
<span id="392"> 392</span>
<span id="393"> 393</span>
<span id="394"> 394</span>
<span id="395"> 395</span>
<span id="396"> 396</span>
<span id="397"> 397</span>
<span id="398"> 398</span>
<span id="399"> 399</span>
<span id="400"> 400</span>
<span id="401"> 401</span>
<span id="402"> 402</span>
<span id="403"> 403</span>
<span id="404"> 404</span>
<span id="405"> 405</span>
<span id="406"> 406</span>
<span id="407"> 407</span>
<span id="408"> 408</span>
<span id="409"> 409</span>
<span id="410"> 410</span>
<span id="411"> 411</span>
<span id="412"> 412</span>
<span id="413"> 413</span>
<span id="414"> 414</span>
<span id="415"> 415</span>
<span id="416"> 416</span>
<span id="417"> 417</span>
<span id="418"> 418</span>
<span id="419"> 419</span>
<span id="420"> 420</span>
<span id="421"> 421</span>
<span id="422"> 422</span>
<span id="423"> 423</span>
<span id="424"> 424</span>
<span id="425"> 425</span>
<span id="426"> 426</span>
<span id="427"> 427</span>
<span id="428"> 428</span>
<span id="429"> 429</span>
<span id="430"> 430</span>
<span id="431"> 431</span>
<span id="432"> 432</span>
<span id="433"> 433</span>
<span id="434"> 434</span>
<span id="435"> 435</span>
<span id="436"> 436</span>
<span id="437"> 437</span>
<span id="438"> 438</span>
<span id="439"> 439</span>
<span id="440"> 440</span>
<span id="441"> 441</span>
<span id="442"> 442</span>
<span id="443"> 443</span>
<span id="444"> 444</span>
<span id="445"> 445</span>
<span id="446"> 446</span>
<span id="447"> 447</span>
<span id="448"> 448</span>
<span id="449"> 449</span>
<span id="450"> 450</span>
<span id="451"> 451</span>
<span id="452"> 452</span>
<span id="453"> 453</span>
<span id="454"> 454</span>
<span id="455"> 455</span>
<span id="456"> 456</span>
<span id="457"> 457</span>
<span id="458"> 458</span>
<span id="459"> 459</span>
<span id="460"> 460</span>
<span id="461"> 461</span>
<span id="462"> 462</span>
<span id="463"> 463</span>
<span id="464"> 464</span>
<span id="465"> 465</span>
<span id="466"> 466</span>
<span id="467"> 467</span>
<span id="468"> 468</span>
<span id="469"> 469</span>
<span id="470"> 470</span>
<span id="471"> 471</span>
<span id="472"> 472</span>
<span id="473"> 473</span>
<span id="474"> 474</span>
<span id="475"> 475</span>
<span id="476"> 476</span>
<span id="477"> 477</span>
<span id="478"> 478</span>
<span id="479"> 479</span>
<span id="480"> 480</span>
<span id="481"> 481</span>
<span id="482"> 482</span>
<span id="483"> 483</span>
<span id="484"> 484</span>
<span id="485"> 485</span>
<span id="486"> 486</span>
<span id="487"> 487</span>
<span id="488"> 488</span>
<span id="489"> 489</span>
<span id="490"> 490</span>
<span id="491"> 491</span>
<span id="492"> 492</span>
<span id="493"> 493</span>
<span id="494"> 494</span>
<span id="495"> 495</span>
<span id="496"> 496</span>
<span id="497"> 497</span>
<span id="498"> 498</span>
<span id="499"> 499</span>
<span id="500"> 500</span>
<span id="501"> 501</span>
<span id="502"> 502</span>
<span id="503"> 503</span>
<span id="504"> 504</span>
<span id="505"> 505</span>
<span id="506"> 506</span>
<span id="507"> 507</span>
<span id="508"> 508</span>
<span id="509"> 509</span>
<span id="510"> 510</span>
<span id="511"> 511</span>
<span id="512"> 512</span>
<span id="513"> 513</span>
<span id="514"> 514</span>
<span id="515"> 515</span>
<span id="516"> 516</span>
<span id="517"> 517</span>
<span id="518"> 518</span>
<span id="519"> 519</span>
<span id="520"> 520</span>
<span id="521"> 521</span>
<span id="522"> 522</span>
<span id="523"> 523</span>
<span id="524"> 524</span>
<span id="525"> 525</span>
<span id="526"> 526</span>
<span id="527"> 527</span>
<span id="528"> 528</span>
<span id="529"> 529</span>
<span id="530"> 530</span>
<span id="531"> 531</span>
<span id="532"> 532</span>
<span id="533"> 533</span>
<span id="534"> 534</span>
<span id="535"> 535</span>
<span id="536"> 536</span>
<span id="537"> 537</span>
<span id="538"> 538</span>
<span id="539"> 539</span>
<span id="540"> 540</span>
<span id="541"> 541</span>
<span id="542"> 542</span>
<span id="543"> 543</span>
<span id="544"> 544</span>
<span id="545"> 545</span>
<span id="546"> 546</span>
<span id="547"> 547</span>
<span id="548"> 548</span>
<span id="549"> 549</span>
<span id="550"> 550</span>
<span id="551"> 551</span>
<span id="552"> 552</span>
<span id="553"> 553</span>
<span id="554"> 554</span>
<span id="555"> 555</span>
<span id="556"> 556</span>
<span id="557"> 557</span>
<span id="558"> 558</span>
<span id="559"> 559</span>
<span id="560"> 560</span>
<span id="561"> 561</span>
<span id="562"> 562</span>
<span id="563"> 563</span>
<span id="564"> 564</span>
<span id="565"> 565</span>
<span id="566"> 566</span>
<span id="567"> 567</span>
<span id="568"> 568</span>
<span id="569"> 569</span>
<span id="570"> 570</span>
<span id="571"> 571</span>
<span id="572"> 572</span>
<span id="573"> 573</span>
<span id="574"> 574</span>
<span id="575"> 575</span>
<span id="576"> 576</span>
<span id="577"> 577</span>
<span id="578"> 578</span>
<span id="579"> 579</span>
<span id="580"> 580</span>
<span id="581"> 581</span>
<span id="582"> 582</span>
<span id="583"> 583</span>
<span id="584"> 584</span>
<span id="585"> 585</span>
<span id="586"> 586</span>
<span id="587"> 587</span>
<span id="588"> 588</span>
<span id="589"> 589</span>
<span id="590"> 590</span>
<span id="591"> 591</span>
<span id="592"> 592</span>
<span id="593"> 593</span>
<span id="594"> 594</span>
<span id="595"> 595</span>
<span id="596"> 596</span>
<span id="597"> 597</span>
<span id="598"> 598</span>
<span id="599"> 599</span>
<span id="600"> 600</span>
<span id="601"> 601</span>
<span id="602"> 602</span>
<span id="603"> 603</span>
<span id="604"> 604</span>
<span id="605"> 605</span>
<span id="606"> 606</span>
<span id="607"> 607</span>
<span id="608"> 608</span>
<span id="609"> 609</span>
<span id="610"> 610</span>
<span id="611"> 611</span>
<span id="612"> 612</span>
<span id="613"> 613</span>
<span id="614"> 614</span>
<span id="615"> 615</span>
<span id="616"> 616</span>
<span id="617"> 617</span>
<span id="618"> 618</span>
<span id="619"> 619</span>
<span id="620"> 620</span>
<span id="621"> 621</span>
<span id="622"> 622</span>
<span id="623"> 623</span>
<span id="624"> 624</span>
<span id="625"> 625</span>
<span id="626"> 626</span>
<span id="627"> 627</span>
<span id="628"> 628</span>
<span id="629"> 629</span>
<span id="630"> 630</span>
<span id="631"> 631</span>
<span id="632"> 632</span>
<span id="633"> 633</span>
<span id="634"> 634</span>
<span id="635"> 635</span>
<span id="636"> 636</span>
<span id="637"> 637</span>
<span id="638"> 638</span>
<span id="639"> 639</span>
<span id="640"> 640</span>
<span id="641"> 641</span>
<span id="642"> 642</span>
<span id="643"> 643</span>
<span id="644"> 644</span>
<span id="645"> 645</span>
<span id="646"> 646</span>
<span id="647"> 647</span>
<span id="648"> 648</span>
<span id="649"> 649</span>
<span id="650"> 650</span>
<span id="651"> 651</span>
<span id="652"> 652</span>
<span id="653"> 653</span>
<span id="654"> 654</span>
<span id="655"> 655</span>
<span id="656"> 656</span>
<span id="657"> 657</span>
<span id="658"> 658</span>
<span id="659"> 659</span>
<span id="660"> 660</span>
<span id="661"> 661</span>
<span id="662"> 662</span>
<span id="663"> 663</span>
<span id="664"> 664</span>
<span id="665"> 665</span>
<span id="666"> 666</span>
<span id="667"> 667</span>
<span id="668"> 668</span>
<span id="669"> 669</span>
<span id="670"> 670</span>
<span id="671"> 671</span>
<span id="672"> 672</span>
<span id="673"> 673</span>
<span id="674"> 674</span>
<span id="675"> 675</span>
<span id="676"> 676</span>
<span id="677"> 677</span>
<span id="678"> 678</span>
<span id="679"> 679</span>
<span id="680"> 680</span>
<span id="681"> 681</span>
<span id="682"> 682</span>
<span id="683"> 683</span>
<span id="684"> 684</span>
<span id="685"> 685</span>
<span id="686"> 686</span>
<span id="687"> 687</span>
<span id="688"> 688</span>
<span id="689"> 689</span>
<span id="690"> 690</span>
<span id="691"> 691</span>
<span id="692"> 692</span>
<span id="693"> 693</span>
<span id="694"> 694</span>
<span id="695"> 695</span>
<span id="696"> 696</span>
<span id="697"> 697</span>
<span id="698"> 698</span>
<span id="699"> 699</span>
<span id="700"> 700</span>
<span id="701"> 701</span>
<span id="702"> 702</span>
<span id="703"> 703</span>
<span id="704"> 704</span>
<span id="705"> 705</span>
<span id="706"> 706</span>
<span id="707"> 707</span>
<span id="708"> 708</span>
<span id="709"> 709</span>
<span id="710"> 710</span>
<span id="711"> 711</span>
<span id="712"> 712</span>
<span id="713"> 713</span>
<span id="714"> 714</span>
<span id="715"> 715</span>
<span id="716"> 716</span>
<span id="717"> 717</span>
<span id="718"> 718</span>
<span id="719"> 719</span>
<span id="720"> 720</span>
<span id="721"> 721</span>
<span id="722"> 722</span>
<span id="723"> 723</span>
<span id="724"> 724</span>
<span id="725"> 725</span>
<span id="726"> 726</span>
<span id="727"> 727</span>
<span id="728"> 728</span>
<span id="729"> 729</span>
<span id="730"> 730</span>
<span id="731"> 731</span>
<span id="732"> 732</span>
<span id="733"> 733</span>
<span id="734"> 734</span>
<span id="735"> 735</span>
<span id="736"> 736</span>
<span id="737"> 737</span>
<span id="738"> 738</span>
<span id="739"> 739</span>
<span id="740"> 740</span>
<span id="741"> 741</span>
<span id="742"> 742</span>
<span id="743"> 743</span>
<span id="744"> 744</span>
<span id="745"> 745</span>
<span id="746"> 746</span>
<span id="747"> 747</span>
<span id="748"> 748</span>
<span id="749"> 749</span>
<span id="750"> 750</span>
<span id="751"> 751</span>
<span id="752"> 752</span>
<span id="753"> 753</span>
<span id="754"> 754</span>
<span id="755"> 755</span>
<span id="756"> 756</span>
<span id="757"> 757</span>
<span id="758"> 758</span>
<span id="759"> 759</span>
<span id="760"> 760</span>
<span id="761"> 761</span>
<span id="762"> 762</span>
<span id="763"> 763</span>
<span id="764"> 764</span>
<span id="765"> 765</span>
<span id="766"> 766</span>
<span id="767"> 767</span>
<span id="768"> 768</span>
<span id="769"> 769</span>
<span id="770"> 770</span>
<span id="771"> 771</span>
<span id="772"> 772</span>
<span id="773"> 773</span>
<span id="774"> 774</span>
<span id="775"> 775</span>
<span id="776"> 776</span>
<span id="777"> 777</span>
<span id="778"> 778</span>
<span id="779"> 779</span>
<span id="780"> 780</span>
<span id="781"> 781</span>
<span id="782"> 782</span>
<span id="783"> 783</span>
<span id="784"> 784</span>
<span id="785"> 785</span>
<span id="786"> 786</span>
<span id="787"> 787</span>
<span id="788"> 788</span>
<span id="789"> 789</span>
<span id="790"> 790</span>
<span id="791"> 791</span>
<span id="792"> 792</span>
<span id="793"> 793</span>
<span id="794"> 794</span>
<span id="795"> 795</span>
<span id="796"> 796</span>
<span id="797"> 797</span>
<span id="798"> 798</span>
<span id="799"> 799</span>
<span id="800"> 800</span>
<span id="801"> 801</span>
<span id="802"> 802</span>
<span id="803"> 803</span>
<span id="804"> 804</span>
<span id="805"> 805</span>
<span id="806"> 806</span>
<span id="807"> 807</span>
<span id="808"> 808</span>
<span id="809"> 809</span>
<span id="810"> 810</span>
<span id="811"> 811</span>
<span id="812"> 812</span>
<span id="813"> 813</span>
<span id="814"> 814</span>
<span id="815"> 815</span>
<span id="816"> 816</span>
<span id="817"> 817</span>
<span id="818"> 818</span>
<span id="819"> 819</span>
<span id="820"> 820</span>
<span id="821"> 821</span>
<span id="822"> 822</span>
<span id="823"> 823</span>
<span id="824"> 824</span>
<span id="825"> 825</span>
<span id="826"> 826</span>
<span id="827"> 827</span>
<span id="828"> 828</span>
<span id="829"> 829</span>
<span id="830"> 830</span>
<span id="831"> 831</span>
<span id="832"> 832</span>
<span id="833"> 833</span>
<span id="834"> 834</span>
<span id="835"> 835</span>
<span id="836"> 836</span>
<span id="837"> 837</span>
<span id="838"> 838</span>
<span id="839"> 839</span>
<span id="840"> 840</span>
<span id="841"> 841</span>
<span id="842"> 842</span>
<span id="843"> 843</span>
<span id="844"> 844</span>
<span id="845"> 845</span>
<span id="846"> 846</span>
<span id="847"> 847</span>
<span id="848"> 848</span>
<span id="849"> 849</span>
<span id="850"> 850</span>
<span id="851"> 851</span>
<span id="852"> 852</span>
<span id="853"> 853</span>
<span id="854"> 854</span>
<span id="855"> 855</span>
<span id="856"> 856</span>
<span id="857"> 857</span>
<span id="858"> 858</span>
<span id="859"> 859</span>
<span id="860"> 860</span>
<span id="861"> 861</span>
<span id="862"> 862</span>
<span id="863"> 863</span>
<span id="864"> 864</span>
<span id="865"> 865</span>
<span id="866"> 866</span>
<span id="867"> 867</span>
<span id="868"> 868</span>
<span id="869"> 869</span>
<span id="870"> 870</span>
<span id="871"> 871</span>
<span id="872"> 872</span>
<span id="873"> 873</span>
<span id="874"> 874</span>
<span id="875"> 875</span>
<span id="876"> 876</span>
<span id="877"> 877</span>
<span id="878"> 878</span>
<span id="879"> 879</span>
<span id="880"> 880</span>
<span id="881"> 881</span>
<span id="882"> 882</span>
<span id="883"> 883</span>
<span id="884"> 884</span>
<span id="885"> 885</span>
<span id="886"> 886</span>
<span id="887"> 887</span>
<span id="888"> 888</span>
<span id="889"> 889</span>
<span id="890"> 890</span>
<span id="891"> 891</span>
<span id="892"> 892</span>
<span id="893"> 893</span>
<span id="894"> 894</span>
<span id="895"> 895</span>
<span id="896"> 896</span>
<span id="897"> 897</span>
<span id="898"> 898</span>
<span id="899"> 899</span>
<span id="900"> 900</span>
<span id="901"> 901</span>
<span id="902"> 902</span>
<span id="903"> 903</span>
<span id="904"> 904</span>
<span id="905"> 905</span>
<span id="906"> 906</span>
<span id="907"> 907</span>
<span id="908"> 908</span>
<span id="909"> 909</span>
<span id="910"> 910</span>
<span id="911"> 911</span>
<span id="912"> 912</span>
<span id="913"> 913</span>
<span id="914"> 914</span>
<span id="915"> 915</span>
<span id="916"> 916</span>
<span id="917"> 917</span>
<span id="918"> 918</span>
<span id="919"> 919</span>
<span id="920"> 920</span>
<span id="921"> 921</span>
<span id="922"> 922</span>
<span id="923"> 923</span>
<span id="924"> 924</span>
<span id="925"> 925</span>
<span id="926"> 926</span>
<span id="927"> 927</span>
<span id="928"> 928</span>
<span id="929"> 929</span>
<span id="930"> 930</span>
<span id="931"> 931</span>
<span id="932"> 932</span>
<span id="933"> 933</span>
<span id="934"> 934</span>
<span id="935"> 935</span>
<span id="936"> 936</span>
<span id="937"> 937</span>
<span id="938"> 938</span>
<span id="939"> 939</span>
<span id="940"> 940</span>
<span id="941"> 941</span>
<span id="942"> 942</span>
<span id="943"> 943</span>
<span id="944"> 944</span>
<span id="945"> 945</span>
<span id="946"> 946</span>
<span id="947"> 947</span>
<span id="948"> 948</span>
<span id="949"> 949</span>
<span id="950"> 950</span>
<span id="951"> 951</span>
<span id="952"> 952</span>
<span id="953"> 953</span>
<span id="954"> 954</span>
<span id="955"> 955</span>
<span id="956"> 956</span>
<span id="957"> 957</span>
<span id="958"> 958</span>
<span id="959"> 959</span>
<span id="960"> 960</span>
<span id="961"> 961</span>
<span id="962"> 962</span>
<span id="963"> 963</span>
<span id="964"> 964</span>
<span id="965"> 965</span>
<span id="966"> 966</span>
<span id="967"> 967</span>
<span id="968"> 968</span>
<span id="969"> 969</span>
<span id="970"> 970</span>
<span id="971"> 971</span>
<span id="972"> 972</span>
<span id="973"> 973</span>
<span id="974"> 974</span>
<span id="975"> 975</span>
<span id="976"> 976</span>
<span id="977"> 977</span>
<span id="978"> 978</span>
<span id="979"> 979</span>
<span id="980"> 980</span>
<span id="981"> 981</span>
<span id="982"> 982</span>
<span id="983"> 983</span>
<span id="984"> 984</span>
<span id="985"> 985</span>
<span id="986"> 986</span>
<span id="987"> 987</span>
<span id="988"> 988</span>
<span id="989"> 989</span>
<span id="990"> 990</span>
<span id="991"> 991</span>
<span id="992"> 992</span>
<span id="993"> 993</span>
<span id="994"> 994</span>
<span id="995"> 995</span>
<span id="996"> 996</span>
<span id="997"> 997</span>
<span id="998"> 998</span>
<span id="999"> 999</span>
<span id="1000">1000</span>
<span id="1001">1001</span>
<span id="1002">1002</span>
<span id="1003">1003</span>
<span id="1004">1004</span>
<span id="1005">1005</span>
<span id="1006">1006</span>
<span id="1007">1007</span>
</pre><pre class='rust '>
<span class='doccomment'>//! Provides the container of a Deep Learning Network</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! Holds all the information about its Layers, how they are connected</span>
<span class='doccomment'>//! and how the [forward][1] and [backward][2] steps should be</span>
<span class='doccomment'>//! handeled and optimized (e.g. skipping layers).</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! [1]: ./struct.Network.html#method.forward</span>
<span class='doccomment'>//! [2]: ./struct.Network.html#method.backward</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! If you are looking to train/test a network, [Solver][3] is usually a better</span>
<span class='doccomment'>//! entry point.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Development</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! Currently only new networks can be created with [from_config][4].</span>
<span class='doccomment'>//! In the future there should also be a way to load networks with saved</span>
<span class='doccomment'>//! weights from a file.</span>
<span class='doccomment'>//! [Issue #14][5].</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! Currently the layers are stored in an array and the metadata in another</span>
<span class='doccomment'>//! array with matching</span>
<span class='doccomment'>//! indices.</span>
<span class='doccomment'>//! In the future we would like to take the metadata into the Layer struct.</span>
<span class='doccomment'>//! [Issue #16][6].</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! [3]: ../solver/index.html</span>
<span class='doccomment'>//! [4]: #method.from_config</span>
<span class='doccomment'>//! [5]: https://github.com/autumnai/leaf/issues/14</span>
<span class='doccomment'>//! [6]: https://github.com/autumnai/leaf/issues/16</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Glossary</span>
<span class='doccomment'>//! ### Input Layers / Blobs</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! A input layer is the bottom-most layer of a network.&lt;/br&gt;</span>
<span class='doccomment'>//! During a forward step the data is put into the input layer,</span>
<span class='doccomment'>//! passed through all the intermediate (hidden) layers and generates a</span>
<span class='doccomment'>//! result in the output layer.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! The blobs in a input layer contain externally preprocessed data that has</span>
<span class='doccomment'>//! been brought</span>
<span class='doccomment'>//! into a form suitable for consumption by a neural network.</span>
<span class='doccomment'>//! TODO: explanation about feedforward / backpropagation</span>
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>collections</span>::{<span class='ident'>HashMap</span>, <span class='ident'>HashSet</span>};
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>sync</span>::{<span class='ident'>Arc</span>, <span class='ident'>RwLock</span>};
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>cmp</span>;
<span class='kw'>use</span> <span class='ident'>math</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>shared_memory</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>layer</span>::{<span class='ident'>ILayer</span>, <span class='ident'>Layer</span>};
<span class='kw'>use</span> <span class='ident'>layer</span>::{<span class='ident'>LayerConfig</span>, <span class='ident'>WeightConfig</span>};
<span class='kw'>use</span> <span class='ident'>phloem</span>::<span class='ident'>Blob</span>;

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>)]</span>
<span class='doccomment'>/// Defines a [Network][1] that contains the [Layers][2] and [Blobs][3] that store</span>
<span class='doccomment'>/// the intermediate results between the layers which are generated by [forward][4]/[backward][5].</span>
<span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Artificial_neural_network</span>
<span class='doccomment'>/// [2]: ../layer/struct.Layer.html</span>
<span class='doccomment'>/// [3]: ../../phloem/blob/struct.Blob.html</span>
<span class='doccomment'>/// [4]: ./struct.Network.html#method.forward</span>
<span class='doccomment'>/// [5]: ./struct.Network.html#method.backward</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// It is also responsible for setting up the connections between the layers.</span>
<span class='doccomment'>/// A Network is usually used together with a [Solver][6] to optimize the networks&#39; weights.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// [6]: ../solver/struct.Solver.html</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>Network</span> {
    <span class='doccomment'>/// Identifies the Network</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The name is mainly used for logging purposes.</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,
    <span class='ident'>layers</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Layer</span><span class='op'>&gt;</span>,
    <span class='ident'>layer_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
    <span class='ident'>layer_names_index</span>: <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>,
    <span class='ident'>layer_need_backwards</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;</span>,

    <span class='ident'>blobs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>, <span class='comment'>// the blobs storing intermediate results between the layer.</span>
    <span class='ident'>blob_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
    <span class='ident'>blob_names_index</span>: <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>,
    <span class='ident'>blob_need_backwards</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;</span>,

    <span class='ident'>output_blobs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>output_blob_indices</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='comment'>// stores the vectors containing the tops (output in forward) for each layer</span>
    <span class='comment'>// (only references to the blobs)</span>
    <span class='ident'>top_vecs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>top_id_vecs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,
    <span class='comment'>// stores the vectors containing the bottoms (input in forward) for each layer</span>
    <span class='ident'>bottom_vecs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>bottom_id_vecs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>bottom_need_backwards</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;&gt;</span>,

    <span class='ident'>input_blobs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>input_blob_indices</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='comment'>// Vector of weight in the loss (or objective) function of each net blob, indexed by blob_id.</span>
    <span class='ident'>blob_loss_weights</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>,

    <span class='ident'>weight_id_vecs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>weight_owners</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>weight_display_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
    <span class='ident'>weight_layer_indices</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span>(<span class='ident'>usize</span>, <span class='ident'>usize</span>)<span class='op'>&gt;</span>,
    <span class='ident'>weight_names_index</span>: <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Defines the [parameters/weights][1] of the network.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Parameters are currently in the process of being renamed to weights throughout the codebase.</span>
    <span class='doccomment'>/// [Issue #17](https://github.com/autumnai/leaf/issues/17)</span>
    <span class='ident'>weights</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>learnable_weights</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>learnable_weight_ids</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='ident'>weights_lr</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>weights_weight_decay</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>Network</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>Network</span> {
        <span class='ident'>Network</span> {
            <span class='ident'>name</span>: <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>(),
            <span class='ident'>layers</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>layer_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>layer_names_index</span>: <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>::<span class='ident'>new</span>(),
            <span class='ident'>layer_need_backwards</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>blobs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>blob_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>blob_names_index</span>: <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>::<span class='ident'>new</span>(),
            <span class='ident'>blob_need_backwards</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>output_blobs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>output_blob_indices</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>top_vecs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>top_id_vecs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>bottom_vecs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>bottom_id_vecs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>bottom_need_backwards</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>input_blobs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>input_blob_indices</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>blob_loss_weights</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>weight_id_vecs</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_owners</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_display_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_layer_indices</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_names_index</span>: <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>::<span class='ident'>new</span>(),

            <span class='ident'>weights</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>learnable_weights</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>learnable_weight_ids</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>weights_lr</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weights_weight_decay</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
        }
    }
}

<span class='kw'>impl</span> <span class='ident'>Network</span> {
    <span class='doccomment'>/// Creates a Network from a [NetworkConfig][1].</span>
    <span class='doccomment'>/// [1]: ./struct.NetworkConfig.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ## Examples</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ```</span>
    <span class='doccomment'>/// # use leaf::network::*;</span>
    <span class='doccomment'>/// let cfg = NetworkConfig::default();</span>
    <span class='doccomment'>/// Network::from_config(&amp;cfg);</span>
    <span class='doccomment'>/// ```</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>from_config</span>(<span class='ident'>param</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>) <span class='op'>-&gt;</span> <span class='ident'>Network</span> {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>network</span> <span class='op'>=</span> <span class='ident'>Network</span>::<span class='ident'>default</span>();
        <span class='ident'>network</span>.<span class='ident'>init</span>(<span class='ident'>param</span>);
        <span class='ident'>network</span>
    }

    <span class='doccomment'>/// Initializes a network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Sets up the whole structure of the network. It reads the supplied [NetworkConfig][1],</span>
    <span class='doccomment'>/// appends the top and bottom blobs to each layer and determines if the backpropagation has</span>
    <span class='doccomment'>/// to be executed for each blob and layer.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: ./struct.NetworkConfig.html</span>
    <span class='kw'>fn</span> <span class='ident'>init</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>in_config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>) {
        <span class='kw'>let</span> <span class='ident'>config</span> <span class='op'>=</span> <span class='ident'>in_config</span>.<span class='ident'>clone</span>();
        <span class='kw'>let</span> <span class='ident'>available_blobs</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span>::<span class='ident'>new</span>();
        <span class='kw'>let</span> <span class='ident'>blob_name_to_idx</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='kw'>for</span> (<span class='ident'>input_id</span>, _) <span class='kw'>in</span> <span class='ident'>config</span>.<span class='ident'>inputs</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>append_top</span>(<span class='kw-2'>&amp;</span><span class='ident'>config</span>,
                            <span class='prelude-val'>None</span>,
                            <span class='ident'>input_id</span>,
                            <span class='prelude-val'>Some</span>(<span class='ident'>available_blobs</span>),
                            <span class='prelude-val'>Some</span>(<span class='ident'>blob_name_to_idx</span>));
        }

        <span class='self'>self</span>.<span class='ident'>resize_vecs</span>(<span class='ident'>config</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>());

        <span class='kw'>for</span> (<span class='ident'>layer_id</span>, _) <span class='kw'>in</span> <span class='ident'>config</span>.<span class='ident'>inputs</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>init_layer</span>(<span class='ident'>layer_id</span>, <span class='kw-2'>&amp;</span><span class='ident'>config</span>, <span class='ident'>available_blobs</span>, <span class='ident'>blob_name_to_idx</span>);
        }

        <span class='comment'>// Go through the net backwards to determine which blobs contribute to the</span>
        <span class='comment'>// loss.  We can skip backward computation for blobs that don&#39;t contribute</span>
        <span class='comment'>// to the loss.</span>
        <span class='comment'>// Also checks if all bottom blobs don&#39;t need backward computation (possible</span>
        <span class='comment'>// because the skip_propagate_down config) and so we can skip backward</span>
        <span class='comment'>// computation for the entire layer</span>
        <span class='kw'>let</span> <span class='ident'>blobs_under_loss</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span>::<span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='kw'>let</span> <span class='ident'>blobs_skip_backp</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span>::<span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='comment'>// get mutable references to struct fields because Rust doesn&#39;t support</span>
        <span class='comment'>// partially borrowed structs</span>
        <span class='kw'>let</span> <span class='ident'>layer_need_backwards</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>layer_need_backwards</span>.<span class='ident'>clone</span>();
        <span class='kw'>let</span> <span class='ident'>bottom_need_backwards</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>.<span class='ident'>clone</span>();
        <span class='kw'>for</span> (<span class='ident'>layer_id</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>iter</span>().<span class='ident'>rev</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>init_backprop</span>(<span class='ident'>layer_id</span>,
                               <span class='ident'>layer_need_backwards</span>,
                               <span class='ident'>bottom_need_backwards</span>,
                               <span class='ident'>blobs_under_loss</span>,
                               <span class='ident'>blobs_skip_backp</span>);
        }

        <span class='kw'>if</span> <span class='ident'>config</span>.<span class='ident'>force_backward</span> {
            <span class='self'>self</span>.<span class='ident'>init_force_backward</span>();
        }

        <span class='comment'>// In the end, all remaining blobs are considered output blobs.</span>
        <span class='kw'>for</span> <span class='ident'>available_blob</span> <span class='kw'>in</span> <span class='ident'>available_blobs</span>.<span class='ident'>iter</span>() {
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;This network produces output {}&quot;</span>, <span class='ident'>available_blob</span>);
            <span class='kw'>let</span> <span class='ident'>id</span> <span class='op'>=</span> <span class='ident'>blob_name_to_idx</span>[<span class='ident'>available_blob</span>];
            <span class='self'>self</span>.<span class='ident'>output_blobs</span>.<span class='ident'>push</span>(<span class='self'>self</span>.<span class='ident'>blobs</span>[<span class='ident'>id</span>].<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>output_blob_indices</span>.<span class='ident'>push</span>(<span class='ident'>id</span>);
        }

        <span class='comment'>// setup names-&gt;idx</span>
        <span class='kw'>for</span> (<span class='ident'>blob_id</span>, <span class='ident'>blob_name</span>) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>blob_names</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>blob_names_index</span>.<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>clone</span>(), <span class='ident'>blob_id</span>);
        }
        <span class='kw'>for</span> (<span class='ident'>layer_id</span>, <span class='ident'>layer_name</span>) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>layer_names</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>layer_names_index</span>.<span class='ident'>insert</span>(<span class='ident'>layer_name</span>.<span class='ident'>clone</span>(), <span class='ident'>layer_id</span>);
        }

        <span class='self'>self</span>.<span class='ident'>share_weights</span>();

        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Network initialization done.&quot;</span>);
    }

    <span class='doccomment'>/// Initializes a single layer of the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Appends [top][1] and [bottom blobs][2] to the [Layer][3]. Apart from explicitly named</span>
    <span class='doccomment'>/// top blobs it will also append anonymous top blobs that are required by the specific</span>
    <span class='doccomment'>/// [Layer implemenations][4]. It also sets up the [loss weights],</span>
    <span class='doccomment'>/// and backpropagation flags.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: ../layer/index.html</span>
    <span class='doccomment'>/// [2]: ../layer/index.html</span>
    <span class='doccomment'>/// [3]: ../layer/struct.Layer.html</span>
    <span class='doccomment'>/// [4]: ../layers/index.html</span>
    <span class='kw'>fn</span> <span class='ident'>init_layer</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                  <span class='ident'>layer_id</span>: <span class='ident'>usize</span>,
                  <span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>,
                  <span class='ident'>available_blobs</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
                  <span class='ident'>blob_name_to_idx</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>) {
        <span class='comment'>// Caffe</span>
        <span class='comment'>// bool share_from_root = !Caffe::root_solver()</span>
        <span class='comment'>//     &amp;&amp; root_net_-&gt;layers_[layer_id]-&gt;ShareInParallel();</span>
        <span class='comment'>// // Inherit mode from net if unset.</span>
        <span class='comment'>// if (!param.layer(layer_id).has_mode()) {</span>
        <span class='comment'>//   param.mutable_layer(layer_id)-&gt;set_mode(mode_);</span>
        <span class='comment'>// }</span>

        <span class='comment'>// Setup layer.</span>
        <span class='kw'>let</span> <span class='ident'>layer_config</span> <span class='op'>=</span> (<span class='kw-2'>&amp;</span><span class='ident'>config</span>.<span class='ident'>layers</span>[<span class='ident'>layer_id</span>]).<span class='ident'>clone</span>(); <span class='comment'>// TODO: should be safer</span>
        <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>layer_config</span>.<span class='ident'>check_propagate_down_len</span>() {
            <span class='comment'>// TODO: move layer validation to layer</span>
            <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;propagate_down config must be specified either 0 or bottom_size times&quot;</span>)
        }

        <span class='comment'>// Caffe</span>
        <span class='comment'>// if (share_from_root) {</span>
        <span class='comment'>//   LOG(INFO) &lt;&lt; &quot;Sharing layer &quot; &lt;&lt; layer_param.name() &lt;&lt; &quot; from root net&quot;;</span>
        <span class='comment'>//   layers_.push_back(root_net_-&gt;layers_[layer_id]);</span>
        <span class='comment'>//   layers_[layer_id]-&gt;SetShared(true);</span>
        <span class='comment'>// else {</span>
        {
            <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>push</span>(<span class='ident'>Layer</span>::<span class='ident'>from_config</span>(<span class='kw-2'>&amp;</span><span class='ident'>layer_config</span>));
        }
        <span class='self'>self</span>.<span class='ident'>layer_names</span>.<span class='ident'>push</span>(<span class='ident'>layer_config</span>.<span class='ident'>name</span>.<span class='ident'>clone</span>());
        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Creating Layer {}&quot;</span>, <span class='ident'>layer_config</span>.<span class='ident'>name</span>.<span class='ident'>clone</span>());
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>need_backward</span> <span class='op'>=</span> <span class='boolval'>false</span>;

        <span class='comment'>// Figure out this layer&#39;s input and output</span>

        <span class='kw'>for</span> <span class='ident'>bottom_id</span> <span class='kw'>in</span> <span class='number'>0</span>..(<span class='ident'>layer_config</span>.<span class='ident'>bottoms_len</span>() <span class='op'>-</span> <span class='number'>1</span>) {
            <span class='kw'>let</span> <span class='ident'>blob_id</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>append_bottom</span>(<span class='ident'>config</span>,
                                             <span class='ident'>layer_id</span>,
                                             <span class='ident'>bottom_id</span>,
                                             <span class='ident'>available_blobs</span>,
                                             <span class='ident'>blob_name_to_idx</span>);

            <span class='comment'>// If a blob needs backward, this layer should provide it.</span>
            <span class='ident'>need_backward</span> <span class='op'>|=</span> <span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>[<span class='ident'>blob_id</span>];
        }
        <span class='kw'>let</span> <span class='ident'>num_top</span> <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>tops_len</span>();
        <span class='kw'>for</span> <span class='ident'>top_id</span> <span class='kw'>in</span> <span class='number'>0</span>..(<span class='ident'>num_top</span> <span class='op'>-</span> <span class='number'>1</span>) {
            <span class='self'>self</span>.<span class='ident'>append_top</span>(<span class='ident'>config</span>,
                            <span class='prelude-val'>Some</span>(<span class='ident'>layer_id</span>),
                            <span class='ident'>top_id</span>,
                            <span class='prelude-val'>Some</span>(<span class='ident'>available_blobs</span>),
                            <span class='prelude-val'>Some</span>(<span class='ident'>blob_name_to_idx</span>))
        }

        <span class='comment'>// If the layer specifies that AutoTopBlobs() -&gt; true and the LayerParameter</span>
        <span class='comment'>// specified fewer than the required number (as specified by</span>
        <span class='comment'>// ExactNumTopBlobs() or MinTopBlobs()), allocate them here.</span>
        <span class='kw'>let</span> <span class='ident'>auto_top_blobs</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>worker</span>.<span class='ident'>auto_top_blobs</span>();
        <span class='kw'>let</span> <span class='ident'>min_top_blobs</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>worker</span>.<span class='ident'>min_top_blobs</span>();
        <span class='kw'>let</span> <span class='ident'>exact_num_top_blobs</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>worker</span>.<span class='ident'>exact_num_top_blobs</span>();
        <span class='kw'>if</span> <span class='ident'>auto_top_blobs</span> {
            <span class='kw'>let</span> <span class='ident'>needed_num_top</span> <span class='op'>=</span> <span class='ident'>cmp</span>::<span class='ident'>max</span>(<span class='ident'>min_top_blobs</span>, <span class='ident'>exact_num_top_blobs</span>);
            <span class='kw'>for</span> _ <span class='kw'>in</span> <span class='number'>0</span>..(<span class='ident'>needed_num_top</span> <span class='op'>-</span> <span class='ident'>num_top</span>) {
                <span class='comment'>// Add &quot;anonymous&quot; top blobs -- do not modify available_blobs or</span>
                <span class='comment'>// blob_name_to_idx as we don&#39;t want these blobs to be usable as input</span>
                <span class='comment'>// to other layers.</span>
                <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Adding anonymous top blob&quot;</span>);
                <span class='self'>self</span>.<span class='ident'>append_top</span>(<span class='ident'>config</span>, <span class='prelude-val'>Some</span>(<span class='ident'>layer_id</span>), <span class='ident'>num_top</span>, <span class='prelude-val'>None</span>, <span class='prelude-val'>None</span>);
            }
        }

        <span class='comment'>// After this layer is connected, set it up.</span>
        <span class='comment'>// Caffe</span>
        <span class='comment'>// if (share_from_root) {</span>
        <span class='comment'>//   // Set up size of top blobs using root_net_</span>
        <span class='comment'>//   const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; base_top = root_net_-&gt;top_vecs_[layer_id];</span>
        <span class='comment'>//   const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; this_top = this-&gt;top_vecs_[layer_id];</span>
        <span class='comment'>//   for (int top_id = 0; top_id &lt; base_top.size(); ++top_id) {</span>
        <span class='comment'>//     this_top[top_id]-&gt;ReshapeLike(*base_top[top_id]);</span>
        <span class='comment'>//     LOG(INFO) &lt;&lt; &quot;Created top blob &quot; &lt;&lt; top_id &lt;&lt; &quot; (shape: &quot;</span>
        <span class='comment'>//         &lt;&lt; this_top[top_id]-&gt;shape_string() &lt;&lt;  &quot;) for shared layer &quot;</span>
        <span class='comment'>//         &lt;&lt; layer_param.name();</span>
        <span class='comment'>//   }</span>
        <span class='comment'>// } else {</span>
        {
            <span class='comment'>// layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</span>
            <span class='comment'>// TODO</span>
            <span class='comment'>// self.layers[layer_id].set_up(self.bottom_vecs[layer_id],</span>
            <span class='comment'>// self.top_vecs[layer_id]);</span>
        }

        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Setting up {}&quot;</span>, <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>layer_id</span>]);
        <span class='kw'>let</span> <span class='ident'>layer</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>).<span class='ident'>unwrap</span>(); <span class='comment'>// TODO: should be safer?</span>
        <span class='kw'>for</span> <span class='ident'>top_id</span> <span class='kw'>in</span> <span class='number'>0</span>..(<span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>len</span>() <span class='op'>-</span> <span class='number'>1</span>) {
            <span class='kw'>if</span> <span class='self'>self</span>.<span class='ident'>blob_loss_weights</span>.<span class='ident'>len</span>() <span class='op'>&lt;=</span> <span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>] {
                <span class='self'>self</span>.<span class='ident'>blob_loss_weights</span>.<span class='ident'>resize</span>(<span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>] <span class='op'>+</span> <span class='number'>1</span>, <span class='number'>0f32</span>);
            }
            <span class='self'>self</span>.<span class='ident'>blob_loss_weights</span>[<span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>]] <span class='op'>=</span> <span class='op'>*</span><span class='ident'>layer</span>.<span class='ident'>loss</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>();
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Top shape: {}&quot;</span>,
                  <span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>shape_string</span>());
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;   with loss weight {}&quot;</span>, <span class='op'>*</span><span class='ident'>layer</span>.<span class='ident'>loss</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>());
        }

        <span class='comment'>// TODO: only needed if we allow blobs to be passed along in the layer_config</span>
        <span class='comment'>// const int param_size = layer_param.param_size();</span>
        <span class='comment'>// const int num_param_blobs = layers_[layer_id]-&gt;blobs().size();</span>
        <span class='comment'>// CHECK_LE(param_size, num_param_blobs)</span>
        <span class='comment'>//     &lt;&lt; &quot;Too many params specified for layer &quot; &lt;&lt; layer_param.name();</span>
        <span class='comment'>// ParamSpec default_param_spec;</span>
        <span class='comment'>// for (int param_id = 0; param_id &lt; num_param_blobs; ++param_id) {</span>
        <span class='comment'>//   const ParamSpec* param_spec = (param_id &lt; param_size) ?</span>
        <span class='comment'>//       &amp;layer_param.param(param_id) : &amp;default_param_spec;</span>
        <span class='comment'>//   const bool param_need_backward = param_spec-&gt;lr_mult() != 0;</span>
        <span class='comment'>//   need_backward |= param_need_backward;</span>
        <span class='comment'>//   layers_[layer_id]-&gt;set_param_propagate_down(param_id,</span>
        <span class='comment'>//                                               param_need_backward);</span>
        <span class='comment'>// }</span>
        <span class='comment'>// for (int param_id = 0; param_id &lt; num_param_blobs; ++param_id) {</span>
        <span class='comment'>//   AppendParam(param, layer_id, param_id);</span>
        <span class='comment'>// }</span>

        <span class='comment'>// Finally, set the backward flag</span>
        <span class='self'>self</span>.<span class='ident'>layer_need_backwards</span>.<span class='ident'>push</span>(<span class='ident'>need_backward</span>);
        <span class='kw'>if</span> <span class='ident'>need_backward</span> {
            <span class='kw'>for</span> <span class='ident'>top_id</span> <span class='kw'>in</span> <span class='number'>0</span>..(<span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>len</span>() <span class='op'>-</span> <span class='number'>1</span>) {
                <span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>[<span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>]] <span class='op'>=</span> <span class='boolval'>true</span>;
            }
        }
    }

    <span class='doccomment'>/// Initializes network for [backpropagation][1]</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Go through all the blobs of a layer to determine which blobs contribute to the</span>
    <span class='doccomment'>/// loss of the next layer. We can skip backward computation for blobs that don&#39;t contribute</span>
    <span class='doccomment'>/// to the loss.</span>
    <span class='doccomment'>/// If all of the blobs skip backpropagation we set a flag to skip backpropagation</span>
    <span class='doccomment'>/// of the whole layer.</span>
    <span class='kw'>fn</span> <span class='ident'>init_backprop</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>,
                     <span class='ident'>layer_id</span>: <span class='ident'>usize</span>,
                     <span class='ident'>layer_need_backwards</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;</span>,
                     <span class='ident'>bottom_need_backwards</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;&gt;</span>,
                     <span class='ident'>blobs_under_loss</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
                     <span class='ident'>blobs_skip_backp</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>) {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>layer_contributes_loss</span> <span class='op'>=</span> <span class='boolval'>false</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>layer_skip_propagate_down</span> <span class='op'>=</span> <span class='boolval'>true</span>;
        <span class='kw'>for</span> (<span class='ident'>top_id</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='kw'>let</span> <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>blob_names</span>[<span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>top_id</span>]].<span class='ident'>clone</span>();

            <span class='comment'>// layer is a loss layer or under a loss layer</span>
            <span class='kw'>if</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>layer_id</span>].<span class='ident'>loss</span>(<span class='ident'>top_id</span>).<span class='ident'>is_some</span>() <span class='op'>||</span> <span class='ident'>blobs_under_loss</span>.<span class='ident'>contains</span>(<span class='kw-2'>&amp;</span><span class='ident'>blob_name</span>) {
                <span class='ident'>layer_contributes_loss</span> <span class='op'>=</span> <span class='boolval'>true</span>;
            }
            <span class='comment'>// layer is not marked to skip backprop TODO: confirm doc</span>
            <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>blobs_skip_backp</span>.<span class='ident'>contains</span>(<span class='kw-2'>&amp;</span><span class='ident'>blob_name</span>) {
                <span class='ident'>layer_skip_propagate_down</span> <span class='op'>=</span> <span class='boolval'>false</span>;
            }
            <span class='comment'>// layer contributes loss to some</span>
            <span class='kw'>if</span> <span class='ident'>layer_contributes_loss</span> <span class='op'>&amp;&amp;</span> <span class='op'>!</span><span class='ident'>layer_skip_propagate_down</span> {
                <span class='kw'>break</span>;
            }
        }

        <span class='comment'>// If this layer can skip backward computation, also all his bottom blobs</span>
        <span class='comment'>// don&#39;t need backpropagation</span>
        <span class='kw'>if</span> <span class='ident'>layer_need_backwards</span>[<span class='ident'>layer_id</span>] <span class='op'>&amp;&amp;</span> <span class='ident'>layer_skip_propagate_down</span> {
            <span class='ident'>layer_need_backwards</span>[<span class='ident'>layer_id</span>] <span class='op'>=</span> <span class='boolval'>false</span>;
            <span class='kw'>for</span> (<span class='ident'>bottom_id</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>bottom_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
                <span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>] <span class='op'>=</span> <span class='boolval'>false</span>;
            }
        }
        <span class='comment'>// layer doesn&#39;t contribute loss so it does not need to be backpropagated</span>
        <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>layer_contributes_loss</span> {
            <span class='ident'>layer_need_backwards</span>[<span class='ident'>layer_id</span>] <span class='op'>=</span> <span class='boolval'>false</span>;
        }
        <span class='comment'>// if (Caffe::root_solver()) { // Caffe</span>
        {
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;{} needs backward computation: {}&quot;</span>,
                  <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>layer_id</span>],
                  <span class='self'>self</span>.<span class='ident'>layer_need_backwards</span>[<span class='ident'>layer_id</span>]);
        }

        <span class='kw'>for</span> (<span class='ident'>bottom_id</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>bottom_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='kw'>let</span> <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>blob_names</span>[<span class='self'>self</span>.<span class='ident'>bottom_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>]];
            <span class='kw'>if</span> <span class='ident'>layer_contributes_loss</span> {
                <span class='ident'>blobs_under_loss</span>.<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>clone</span>());
            } <span class='kw'>else</span> {
                <span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>] <span class='op'>=</span> <span class='boolval'>false</span>;
            }
            <span class='kw'>if</span> <span class='op'>!</span><span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>] {
                <span class='ident'>blobs_skip_backp</span>.<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>clone</span>());
            }
        }
    }

    <span class='doccomment'>/// Set [backpropagation][1] flags to force all layers to backpropagate.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Is executed during Network initalization if [NetworkConfig][2].force_backward is true.</span>
    <span class='doccomment'>/// Forcing backpropagation is useful for debugging.</span>
    <span class='kw'>fn</span> <span class='ident'>init_force_backward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>for</span> (<span class='ident'>layer_id</span>, <span class='ident'>layer</span>) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>iter_mut</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>layer_need_backwards</span>[<span class='ident'>layer_id</span>] <span class='op'>=</span> <span class='boolval'>true</span>;
            <span class='kw'>for</span> (<span class='ident'>bottom_id</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>].<span class='ident'>clone</span>().<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
                <span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>] <span class='op'>=</span>
                    <span class='op'>*</span><span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>]
                         .<span class='ident'>get</span>(<span class='ident'>bottom_id</span>)
                         .<span class='ident'>unwrap_or</span>(<span class='kw-2'>&amp;</span><span class='ident'>layer</span>.<span class='ident'>worker</span>.<span class='ident'>allow_force_backward</span>(<span class='ident'>bottom_id</span>));
                <span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>[<span class='self'>self</span>.<span class='ident'>bottom_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>]] <span class='op'>=</span>
                    <span class='op'>*</span><span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>
                         .<span class='ident'>get</span>(<span class='self'>self</span>.<span class='ident'>bottom_id_vecs</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>])
                         .<span class='ident'>unwrap_or</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>][<span class='ident'>bottom_id</span>])
            }
            <span class='kw'>for</span> (<span class='ident'>weight_id</span>, _) <span class='kw'>in</span> <span class='ident'>layer</span>.<span class='ident'>blobs</span>.<span class='ident'>clone</span>().<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
                <span class='ident'>layer</span>.<span class='ident'>set_weight_propagate_down</span>(<span class='ident'>weight_id</span>, <span class='boolval'>true</span>);
            }
        }
    }

    <span class='doccomment'>/// Resize Vectors that hold the [HeapBlob][1] references and the layer metadata.</span>
    <span class='doccomment'>/// [1]: ../shared_memory/type.HeapBlob.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Used during Network initalization.</span>
    <span class='doccomment'>/// It is unclear if this provides any (speed) benefit since the Vecs only hold</span>
    <span class='doccomment'>/// references, so reallocation should be cheap.</span>
    <span class='kw'>fn</span> <span class='ident'>resize_vecs</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>new_len</span>: <span class='ident'>usize</span>) {
        <span class='self'>self</span>.<span class='ident'>bottom_vecs</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='ident'>Arc</span>::<span class='ident'>new</span>(<span class='ident'>RwLock</span>::<span class='ident'>new</span>(<span class='ident'>Box</span>::<span class='ident'>new</span>(<span class='ident'>Blob</span>::<span class='ident'>new</span>())))]);
        <span class='self'>self</span>.<span class='ident'>top_vecs</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='ident'>Arc</span>::<span class='ident'>new</span>(<span class='ident'>RwLock</span>::<span class='ident'>new</span>(<span class='ident'>Box</span>::<span class='ident'>new</span>(<span class='ident'>Blob</span>::<span class='ident'>new</span>())))]);
        <span class='self'>self</span>.<span class='ident'>bottom_id_vecs</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='number'>0</span>]);
        <span class='self'>self</span>.<span class='ident'>top_id_vecs</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='number'>0</span>]);
        <span class='self'>self</span>.<span class='ident'>weight_id_vecs</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='number'>0</span>]);
        <span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>.<span class='ident'>resize</span>(<span class='ident'>new_len</span>, <span class='macro'>vec</span><span class='macro'>!</span>[<span class='boolval'>false</span>]);
    }

    <span class='doccomment'>/// Share weights among multiple layers.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Shared weights are usually used for [Siamese networks][1]</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.4792</span>
    <span class='kw'>fn</span> <span class='ident'>share_weights</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='comment'>// Caffe / not sure if ported correctly</span>
        <span class='comment'>// for (int i = 0; i &lt; params_.size(); ++i) {</span>
        <span class='comment'>//     if (param_owners_[i] &lt; 0) { continue; }</span>
        <span class='comment'>//     params_[i]-&gt;ShareData(*params_[param_owners_[i]]);</span>
        <span class='comment'>//     params_[i]-&gt;ShareDiff(*params_[param_owners_[i]]);</span>
        <span class='comment'>// }</span>
        <span class='kw'>for</span> (<span class='ident'>i</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>weights</span>.<span class='ident'>clone</span>().<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>j</span>) <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weight_owners</span>[<span class='ident'>i</span>] {
                <span class='macro'>assert</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>i</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>cpu_data</span>().<span class='ident'>capacity</span>() <span class='op'>==</span>
                        <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>j</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>cpu_data</span>().<span class='ident'>capacity</span>());
                <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>i</span>] <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>j</span>].<span class='ident'>clone</span>(); <span class='comment'>// sharing whole blob?</span>
            }
        }
    }

    <span class='doccomment'>/// ???</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// TODO: [DOC] Why? What is the purpose of this?</span>
    <span class='kw'>fn</span> <span class='ident'>append_top</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                  <span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>,
                  <span class='ident'>layer_id</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,
                  <span class='ident'>top_id</span>: <span class='ident'>usize</span>,
                  <span class='ident'>available_blobs</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;&gt;</span>,
                  <span class='ident'>blob_name_to_idx</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;&gt;</span>) {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>layer_config</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span> <span class='op'>=</span> <span class='prelude-val'>None</span>;
        <span class='kw'>if</span> <span class='ident'>layer_id</span>.<span class='ident'>is_some</span>() {
            <span class='ident'>layer_config</span> <span class='op'>=</span> <span class='ident'>config</span>.<span class='ident'>layer</span>(<span class='ident'>layer_id</span>.<span class='ident'>unwrap</span>());
        }

        <span class='kw'>let</span> <span class='ident'>blob_name</span>: <span class='ident'>String</span>;
        <span class='kw'>match</span> <span class='ident'>layer_config</span> {
            <span class='prelude-val'>Some</span>(<span class='ident'>layer_config</span>) <span class='op'>=&gt;</span> {
                <span class='kw'>if</span> <span class='ident'>layer_config</span>.<span class='ident'>top</span>(<span class='ident'>top_id</span>).<span class='ident'>is_some</span>() {
                    <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='ident'>String</span>::<span class='ident'>from</span>(<span class='ident'>layer_config</span>.<span class='ident'>top</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>clone</span>());
                } <span class='kw'>else</span> {
                    <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='string'>&quot;(automatic)&quot;</span>.<span class='ident'>to_owned</span>();
                }
            }
            <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> {
                <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='ident'>String</span>::<span class='ident'>from</span>(<span class='ident'>config</span>.<span class='ident'>input</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>clone</span>());
            }
        }

        <span class='kw'>if</span> <span class='ident'>blob_name_to_idx</span>.<span class='ident'>is_some</span>() <span class='op'>&amp;&amp;</span> <span class='ident'>layer_config</span>.<span class='ident'>is_some</span>() <span class='op'>&amp;&amp;</span> <span class='ident'>layer_config</span>.<span class='ident'>unwrap</span>().<span class='ident'>bottom</span>(<span class='ident'>top_id</span>).<span class='ident'>is_some</span>() <span class='op'>&amp;&amp;</span>
           <span class='op'>*</span><span class='ident'>layer_config</span>.<span class='ident'>unwrap</span>().<span class='ident'>bottom</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>() <span class='op'>==</span> <span class='ident'>blob_name</span> {
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;{} -&gt; {} (in-place)&quot;</span>, <span class='ident'>layer_config</span>.<span class='ident'>unwrap</span>().<span class='ident'>name</span>, <span class='ident'>blob_name</span>);
            <span class='kw'>let</span> <span class='ident'>idx</span> <span class='op'>=</span> <span class='ident'>blob_name_to_idx</span>.<span class='ident'>unwrap</span>()[<span class='kw-2'>&amp;</span><span class='ident'>blob_name</span>];
            <span class='kw'>let</span> <span class='ident'>blob</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>blobs</span>[<span class='ident'>idx</span>].<span class='ident'>clone</span>();
            <span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>layer_id</span>.<span class='ident'>unwrap</span>()].<span class='ident'>push</span>(<span class='ident'>blob</span>);
            <span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>.<span class='ident'>unwrap</span>()].<span class='ident'>push</span>(<span class='ident'>idx</span>);
        } <span class='kw'>else</span> <span class='kw'>if</span> <span class='ident'>blob_name_to_idx</span>.<span class='ident'>is_some</span>() <span class='op'>&amp;&amp;</span> <span class='ident'>blob_name_to_idx</span>.<span class='ident'>as_ref</span>().<span class='ident'>unwrap</span>().<span class='ident'>get</span>(<span class='kw-2'>&amp;</span><span class='ident'>blob_name</span>).<span class='ident'>is_some</span>() {
            <span class='comment'>// If we are not doing in-place computation but have duplicated blobs, raise an</span>
            <span class='comment'>// error.</span>
            <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;Top blob {} produced by multiple sources.&quot;</span>, <span class='ident'>blob_name</span>);
        } <span class='kw'>else</span> {
            <span class='comment'>// if (Caffe::root_solver()) {</span>
            <span class='kw'>if</span> <span class='boolval'>true</span> {
                <span class='kw'>if</span> <span class='ident'>layer_config</span>.<span class='ident'>is_some</span>() {
                    <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;{} -&gt; {}&quot;</span>, <span class='ident'>layer_config</span>.<span class='ident'>unwrap</span>().<span class='ident'>name</span>, <span class='ident'>blob_name</span>);
                }
                <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Input {} -&gt; {}&quot;</span>, <span class='ident'>top_id</span>, <span class='ident'>blob_name</span>);
            }

            <span class='kw'>let</span> <span class='ident'>blob_pointer</span>: <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;</span> <span class='op'>=</span> <span class='ident'>Arc</span>::<span class='ident'>new</span>(<span class='ident'>RwLock</span>::<span class='ident'>new</span>(<span class='ident'>Box</span>::<span class='ident'>new</span>(<span class='ident'>Blob</span>::<span class='ident'>new</span>())));
            <span class='kw'>let</span> <span class='ident'>blob_id</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>blobs</span>.<span class='ident'>len</span>();
            <span class='self'>self</span>.<span class='ident'>blobs</span>.<span class='ident'>push</span>(<span class='ident'>blob_pointer</span>.<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>blob_names</span>.<span class='ident'>push</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>());
            <span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>.<span class='ident'>push</span>(<span class='boolval'>false</span>);
            <span class='kw'>if</span> <span class='ident'>blob_name_to_idx</span>.<span class='ident'>is_some</span>() {
                <span class='ident'>blob_name_to_idx</span>.<span class='ident'>unwrap</span>().<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>(), <span class='ident'>blob_id</span>);
            }

            <span class='kw'>match</span> <span class='ident'>layer_id</span> {
                <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> {
                    <span class='comment'>// Set the (explicitly specified) dimensions of the input blob.</span>
                    <span class='ident'>blob_pointer</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>reshape</span>(<span class='ident'>config</span>.<span class='ident'>input_shape</span>(<span class='ident'>top_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>clone</span>());

                    <span class='self'>self</span>.<span class='ident'>input_blob_indices</span>.<span class='ident'>push</span>(<span class='ident'>blob_id</span>);
                    <span class='self'>self</span>.<span class='ident'>input_blobs</span>.<span class='ident'>push</span>(<span class='ident'>blob_pointer</span>);
                }
                <span class='prelude-val'>Some</span>(<span class='ident'>layer_id</span>) <span class='op'>=&gt;</span> {
                    <span class='self'>self</span>.<span class='ident'>top_id_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='ident'>blob_id</span>);
                    <span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='ident'>blob_pointer</span>);
                }
            }
        }
        <span class='kw'>if</span> <span class='ident'>available_blobs</span>.<span class='ident'>is_some</span>() {
            <span class='ident'>available_blobs</span>.<span class='ident'>unwrap</span>().<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>());
        }
    }

    <span class='doccomment'>/// Append blob as [bottom blob][1] to a [Layer][2].</span>
    <span class='doccomment'>/// [1]: ../layer/index.html</span>
    <span class='doccomment'>/// [2]: ../layer/struct.Layer.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// During network initalization the blobs will be appended to the [Layer][2]s as per their</span>
    <span class='doccomment'>/// [LayerConfig][3]. It is also determined if a bottom blob skips backpropagation</span>
    <span class='doccomment'>/// from [LayerConfig.propagate_down][3] (see also [init_backprop][5]).</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Currently these things are tracked in metadata arrays: [Issue #16]][4].</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [3]: ../layer/struct.LayerConfig.html</span>
    <span class='doccomment'>/// [4]: https://github.com/autumnai/leaf/issues/16</span>
    <span class='doccomment'>/// [5]: #method.init_backprop</span>
    <span class='kw'>fn</span> <span class='ident'>append_bottom</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                     <span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>,
                     <span class='ident'>layer_id</span>: <span class='ident'>usize</span>,
                     <span class='ident'>bottom_id</span>: <span class='ident'>usize</span>,
                     <span class='ident'>available_blobs</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
                     <span class='ident'>blob_name_to_idx</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>)
                     <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='kw'>let</span> <span class='ident'>layer_config</span> <span class='op'>=</span> <span class='ident'>config</span>.<span class='ident'>layer</span>(<span class='ident'>layer_id</span>).<span class='ident'>unwrap</span>();
        <span class='kw'>let</span> <span class='ident'>blob_name</span> <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>bottom</span>(<span class='ident'>bottom_id</span>).<span class='ident'>unwrap</span>();

        <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>available_blobs</span>.<span class='ident'>contains</span>(<span class='ident'>blob_name</span>) {
            <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;Unknown bottom blob {} (layer &#39;{}&#39;, bottom index {})&quot;</span>,
                   <span class='ident'>blob_name</span>,
                   <span class='ident'>layer_config</span>.<span class='ident'>name</span>,
                   <span class='ident'>bottom_id</span>);
        }

        <span class='kw'>let</span> <span class='ident'>blob_id</span> <span class='op'>=</span> <span class='ident'>blob_name_to_idx</span>[<span class='ident'>blob_name</span>];
        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;{} &lt;- {}&quot;</span>, <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>layer_id</span>], <span class='ident'>blob_name</span>);

        <span class='self'>self</span>.<span class='ident'>bottom_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='self'>self</span>.<span class='ident'>blobs</span>[<span class='ident'>blob_id</span>].<span class='ident'>clone</span>());
        <span class='self'>self</span>.<span class='ident'>bottom_id_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='ident'>blob_id</span>);
        <span class='ident'>available_blobs</span>.<span class='ident'>remove</span>(<span class='ident'>blob_name</span>);

        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>propagate_down</span> <span class='op'>=</span> <span class='boolval'>true</span>;
        <span class='comment'>// Check if the backpropagation on bottom_id should be skipped</span>
        <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>layer_config</span>.<span class='ident'>propagate_down</span>.<span class='ident'>is_empty</span>() {
            <span class='ident'>propagate_down</span> <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>propagate_down</span>[<span class='ident'>bottom_id</span>];
        }
        <span class='kw'>let</span> <span class='ident'>need_backward</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>blob_need_backwards</span>[<span class='ident'>blob_id</span>] <span class='op'>&amp;&amp;</span> <span class='ident'>propagate_down</span>;
        <span class='self'>self</span>.<span class='ident'>bottom_need_backwards</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='ident'>need_backward</span>);

        <span class='ident'>blob_id</span>
    }

    <span class='doccomment'>/// Append a weight blob to the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// During network initalization weight blobs are appended to the correct layers.</span>
    <span class='doccomment'>/// If a layer&#39;s [LayerConfig][1] states that the weights are shared,</span>
    <span class='doccomment'>/// this function also makes sure to set a reference to the other weight blob instead of</span>
    <span class='doccomment'>/// allocating a new one.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: ../layer/struct.LayerConfig.html</span>
    <span class='kw'>fn</span> <span class='ident'>append_weight</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>, <span class='ident'>layer_id</span>: <span class='ident'>usize</span>, <span class='ident'>weight_id</span>: <span class='ident'>usize</span>) {
        <span class='kw'>let</span> <span class='ident'>layer_config</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>layer_id</span>].<span class='ident'>config</span>.<span class='ident'>clone</span>();
        <span class='kw'>let</span> <span class='ident'>weights_len</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weights</span>.<span class='ident'>len</span>();
        <span class='kw'>let</span> <span class='ident'>weight_name</span> <span class='op'>=</span> <span class='kw'>if</span> <span class='ident'>weights_len</span> <span class='op'>&gt;</span> <span class='ident'>weight_id</span> {
            <span class='ident'>layer_config</span>.<span class='ident'>param</span>(<span class='ident'>weight_id</span>).<span class='ident'>unwrap</span>().<span class='ident'>name</span>.<span class='ident'>clone</span>()
        } <span class='kw'>else</span> {
            <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>()
        };

        <span class='comment'>// use weight_name (or weight_id as a fallback) as display_name</span>
        <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>weight_name</span>.<span class='ident'>is_empty</span>() {
            <span class='self'>self</span>.<span class='ident'>weight_display_names</span>.<span class='ident'>push</span>(<span class='ident'>weight_name</span>.<span class='ident'>clone</span>());
        } <span class='kw'>else</span> {
            <span class='self'>self</span>.<span class='ident'>weight_display_names</span>.<span class='ident'>push</span>(<span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;{}&quot;</span>, <span class='ident'>weight_id</span>));
        }

        <span class='comment'>// add to tracking vectors</span>
        <span class='kw'>let</span> <span class='ident'>net_weight_id</span> <span class='op'>=</span> <span class='ident'>weights_len</span>;
        <span class='self'>self</span>.<span class='ident'>weights</span>.<span class='ident'>push</span>(<span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>layer_id</span>].<span class='ident'>blobs</span>[<span class='ident'>weight_id</span>].<span class='ident'>clone</span>());
        <span class='self'>self</span>.<span class='ident'>weight_id_vecs</span>[<span class='ident'>layer_id</span>].<span class='ident'>push</span>(<span class='ident'>net_weight_id</span>);
        <span class='self'>self</span>.<span class='ident'>weight_layer_indices</span>.<span class='ident'>push</span>((<span class='ident'>layer_id</span>, <span class='ident'>weight_id</span>));

        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>weight_config</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='ident'>WeightConfig</span>::<span class='ident'>default</span>();
        <span class='kw'>if</span> <span class='ident'>layer_config</span>.<span class='ident'>params_len</span>() <span class='op'>&gt;</span> <span class='ident'>weight_id</span> {
            <span class='ident'>weight_config</span> <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>param</span>(<span class='ident'>weight_id</span>).<span class='ident'>unwrap</span>();
        }
        <span class='comment'>// This layer &quot;owns&quot; this weight blob -- it is either anonymous</span>
        <span class='comment'>// (i.e., not given a weight_name) or explicitly given a name that we</span>
        <span class='comment'>// haven&#39;t already seen.</span>
        <span class='kw'>if</span> <span class='ident'>weight_name</span>.<span class='ident'>is_empty</span>() <span class='op'>||</span> <span class='op'>!</span><span class='self'>self</span>.<span class='ident'>weight_names_index</span>.<span class='ident'>contains_key</span>(<span class='kw-2'>&amp;</span><span class='ident'>weight_name</span>) {
            <span class='self'>self</span>.<span class='ident'>weight_owners</span>.<span class='ident'>push</span>(<span class='prelude-val'>None</span>);
            <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>weight_name</span>.<span class='ident'>is_empty</span>() {
                <span class='self'>self</span>.<span class='ident'>weight_names_index</span>.<span class='ident'>insert</span>(<span class='ident'>weight_name</span>.<span class='ident'>clone</span>(), <span class='ident'>net_weight_id</span>);
            }
            <span class='kw'>let</span> <span class='ident'>learnable_weight_id</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>learnable_weights</span>.<span class='ident'>len</span>();
            <span class='self'>self</span>.<span class='ident'>learnable_weights</span>.<span class='ident'>push</span>(<span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>net_weight_id</span>].<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>learnable_weight_ids</span>.<span class='ident'>push</span>(<span class='ident'>learnable_weight_id</span>);
            <span class='comment'>//     has_weights_lr_.push_back(weight_config-&gt;has_lr_mult());</span>
            <span class='comment'>//     has_params_decay_.push_back(param_spec-&gt;has_decay_mult());</span>
            <span class='self'>self</span>.<span class='ident'>weights_lr</span>.<span class='ident'>push</span>(<span class='ident'>weight_config</span>.<span class='ident'>lr_mult</span>.<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>weights_weight_decay</span>.<span class='ident'>push</span>(<span class='ident'>weight_config</span>.<span class='ident'>decay_mult</span>.<span class='ident'>clone</span>());
        } <span class='kw'>else</span> {
            <span class='comment'>// Named weight blob with name we&#39;ve seen before: share weights</span>

            <span class='kw'>let</span> <span class='ident'>owner_net_weight_id</span> <span class='op'>=</span> <span class='op'>*</span><span class='self'>self</span>.<span class='ident'>weight_names_index</span>.<span class='ident'>get</span>(<span class='kw-2'>&amp;</span><span class='ident'>weight_name</span>).<span class='ident'>unwrap</span>();
            <span class='self'>self</span>.<span class='ident'>weight_owners</span>.<span class='ident'>push</span>(<span class='prelude-val'>Some</span>(<span class='ident'>owner_net_weight_id</span>));
            <span class='kw'>let</span> (<span class='ident'>owner_layer_id</span>, <span class='ident'>owner_weight_id</span>) <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weight_layer_indices</span>[<span class='ident'>owner_net_weight_id</span>];
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Sharing weights &#39;{}&#39; owned by layer &#39;{}&#39;, weight index {}&quot;</span>,
                  <span class='ident'>weight_name</span>.<span class='ident'>clone</span>(),
                  <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>owner_layer_id</span>],
                  <span class='ident'>owner_weight_id</span>);
            <span class='kw'>let</span> <span class='ident'>this_blob</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>layer_id</span>].<span class='ident'>blobs</span>[<span class='ident'>weight_id</span>].<span class='ident'>clone</span>();
            <span class='kw'>let</span> <span class='ident'>owner_blob</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>owner_layer_id</span>].<span class='ident'>blobs</span>[<span class='ident'>owner_weight_id</span>].<span class='ident'>clone</span>();
            <span class='comment'>// can only share weights if blobs match by shape or capacity</span>
            <span class='kw'>if</span> <span class='ident'>weights_len</span> <span class='op'>&gt;</span> <span class='ident'>weight_id</span> {
                <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Err</span>(<span class='ident'>e</span>) <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>param</span>(<span class='ident'>weight_id</span>)
                                            .<span class='ident'>unwrap</span>()
                                            .<span class='ident'>check_dimensions</span>(<span class='kw-2'>&amp;</span><span class='ident'>this_blob</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>(),
                                                              <span class='kw-2'>&amp;</span><span class='ident'>owner_blob</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>(),
                                                              <span class='ident'>weight_name</span>.<span class='ident'>clone</span>(),
                                                              <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>owner_layer_id</span>].<span class='ident'>clone</span>(),
                                                              <span class='self'>self</span>.<span class='ident'>layer_names</span>[<span class='ident'>layer_id</span>].<span class='ident'>clone</span>()) {
                    <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;{}&quot;</span>, <span class='ident'>e</span>)
                }
            }

            <span class='kw'>let</span> <span class='ident'>learnable_weight_id</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>learnable_weight_ids</span>[<span class='ident'>owner_net_weight_id</span>];
            <span class='self'>self</span>.<span class='ident'>learnable_weight_ids</span>.<span class='ident'>push</span>(<span class='ident'>learnable_weight_id</span>);
            <span class='comment'>// can only share parameters if both have same lr_mult</span>
            <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>lr_mult</span>) <span class='op'>=</span> <span class='ident'>weight_config</span>.<span class='ident'>lr_mult</span> {
                <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>owner_lr_mult</span>) <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weights_lr</span>[<span class='ident'>learnable_weight_id</span>] {
                    <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>lr_mult</span>.<span class='ident'>eq</span>(<span class='kw-2'>&amp;</span><span class='ident'>owner_lr_mult</span>) {
                        <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;Shared param &#39;{}&#39; has mismatched lr_mult.&quot;</span>,
                               <span class='ident'>weight_name</span>.<span class='ident'>clone</span>());
                    }
                } <span class='kw'>else</span> {
                    <span class='self'>self</span>.<span class='ident'>weights_lr</span>[<span class='ident'>learnable_weight_id</span>] <span class='op'>=</span> <span class='ident'>weight_config</span>.<span class='ident'>lr_mult</span>;
                }
            }
            <span class='comment'>// can only share weights if both have same decay_mult</span>
            <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>decay_mult</span>) <span class='op'>=</span> <span class='ident'>weight_config</span>.<span class='ident'>decay_mult</span> {
                <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>owner_decay_mult</span>) <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weights_weight_decay</span>[<span class='ident'>learnable_weight_id</span>] {
                    <span class='kw'>if</span> <span class='op'>!</span><span class='ident'>decay_mult</span>.<span class='ident'>eq</span>(<span class='kw-2'>&amp;</span><span class='ident'>owner_decay_mult</span>) {
                        <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;Shared param &#39;{}&#39; has mismatched decay_mult.&quot;</span>,
                               <span class='ident'>weight_name</span>.<span class='ident'>clone</span>());
                    }
                } <span class='kw'>else</span> {
                    <span class='self'>self</span>.<span class='ident'>weights_weight_decay</span>[<span class='ident'>learnable_weight_id</span>] <span class='op'>=</span> <span class='ident'>weight_config</span>.<span class='ident'>decay_mult</span>;
                }
            }
        }
    }


    <span class='doccomment'>/// Computes [forward][1] and [backward][2] step for the network and returns [the total loss.][3]</span>
    <span class='doccomment'>/// [1]: #method.forward</span>
    <span class='doccomment'>/// [2]: #method.backward</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Used by the [Solver][4] to conveniently compute one [forward- and one backward-propagation</span>
    <span class='doccomment'>/// step][5] together, which is all the network has to do while training it.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [4]: ../solver/struct.Solver.html</span>
    <span class='doccomment'>/// [5]: https://en.wikipedia.org/wiki/Backpropagation#Phase_1:_Propagation</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_backward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>bottom</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;</span>]) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='kw'>let</span> <span class='ident'>loss</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='number'>0f32</span>;

        <span class='self'>self</span>.<span class='ident'>forward</span>(<span class='ident'>bottom</span>, <span class='ident'>loss</span>);
        <span class='comment'>// self.backward();</span>

        <span class='op'>*</span><span class='ident'>loss</span>
    }

    <span class='doccomment'>/// Copies supplied [input Blobs][1] into the network, computes [forward step][2] for the</span>
    <span class='doccomment'>/// network and returns [the output blobs.][3].</span>
    <span class='doccomment'>/// [1]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>/// [2]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Does not actually copy data, only references to the input blobs.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This is the go-to if you just want to feed data to your network and get the corresponding</span>
    <span class='doccomment'>/// output.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;</span>], <span class='ident'>loss</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>f32</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span> {
        <span class='kw'>for</span> (<span class='ident'>i</span>, <span class='ident'>inp</span>) <span class='kw'>in</span> <span class='ident'>input</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>input_blobs</span>[<span class='ident'>i</span>] <span class='op'>=</span> <span class='ident'>inp</span>.<span class='ident'>clone</span>();
        }

        <span class='self'>self</span>.<span class='ident'>forward_prefilled</span>(<span class='prelude-val'>Some</span>(<span class='ident'>loss</span>))
    }

    <span class='doccomment'>/// Computes [forward step][1] for a network whose [input blob][2] references have been set</span>
    <span class='doccomment'>/// and returns [the output blobs.][3]</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [2]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Can be used if you need more control over how to put data into the network (debugging),</span>
    <span class='doccomment'>/// otherwise [forward][4] is the prefered method to forward through the whole network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [4]: #method.forward</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_prefilled</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>loss</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>f32</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>end</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>() <span class='op'>-</span> <span class='number'>1</span>;
        <span class='kw'>match</span> <span class='ident'>loss</span> {
            <span class='prelude-val'>Some</span>(<span class='ident'>loss_result</span>) <span class='op'>=&gt;</span> {
                <span class='comment'>// not sure if loss_result will really be changed</span>
                <span class='op'>*</span><span class='ident'>loss_result</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>forward_from_to</span>(<span class='number'>0</span>, <span class='ident'>end</span>);
            }
            <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> {
                <span class='self'>self</span>.<span class='ident'>forward_from_to</span>(<span class='number'>0</span>, <span class='ident'>end</span>);
            }
        }

        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>output_blobs</span>
    }

    <span class='doccomment'>/// Compute [forward step][1] for a part of (or the whole) network and returns the [total loss][2].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [2]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Computes the forward step from the layer with index `start` to the layer with index `end`</span>
    <span class='doccomment'>/// and return the total [scalar loss][2] over all loss layers.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If you want to compute a foward step for the whole layer</span>
    <span class='doccomment'>/// you should use [forward_prefilled][3].</span>
    <span class='doccomment'>/// Computing a forward on a part of the network is usually only done for debugging purposes.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [3]: #method.forward_prefilled</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_from_to</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>start</span>: <span class='ident'>usize</span>, <span class='ident'>end</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>end</span> <span class='op'>&lt;</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>());

        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>loss</span> <span class='op'>=</span> <span class='number'>0f32</span>;

        <span class='comment'>//  Caffe</span>
        <span class='comment'>//   if (debug_info_) {</span>
        <span class='comment'>//     for (int i = 0; i &lt; net_input_blobs_.size(); ++i) {</span>
        <span class='comment'>//       InputDebugInfo(i);</span>
        <span class='comment'>//     }</span>
        <span class='comment'>//   }</span>

        <span class='kw'>for</span> <span class='ident'>i</span> <span class='kw'>in</span> <span class='ident'>start</span>..<span class='ident'>end</span> {
            <span class='ident'>loss</span> <span class='op'>+=</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>worker</span>.<span class='ident'>forward</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>bottom_vecs</span>[<span class='ident'>i</span>], <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>top_vecs</span>[<span class='ident'>i</span>]);
            <span class='comment'>// if (debug_info_) { ForwardDebugInfo(i); }  // Caffe</span>
        }

        <span class='ident'>loss</span>
    }

    <span class='doccomment'>/// Clears the [weights][1] diffs and zero-inits them.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The diffs for the weights accumulate over the backpropagation steps of</span>
    <span class='doccomment'>/// a [Solver][2] minibatch and are cleared between each minibatch</span>
    <span class='doccomment'>/// to start over with a clean slate.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ../solver/struct.Solver.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>clear_weight_diffs</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>for</span> <span class='ident'>weight_blob</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>learnable_weights</span>.<span class='ident'>iter</span>() {
            <span class='kw'>for</span> <span class='ident'>p</span> <span class='kw'>in</span> <span class='ident'>weight_blob</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>mutable_cpu_diff</span>().<span class='ident'>iter_mut</span>() {
                <span class='op'>*</span><span class='ident'>p</span> <span class='op'>=</span> <span class='number'>0f32</span>;
            }
        }
    }

    <span class='doccomment'>/// Updates the [weights][1] with the weight update computed by the [Solver][2].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>/// [2]: ../solver/struct.Solver.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Updating the weights is the last step of computing a [Solver][2] minibatch.</span>
    <span class='doccomment'>/// The update value is computed in previous steps according to the [learning rate policy][3]</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [3]: ../solver/enum.LRPolicy.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>update_weights</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>for</span> <span class='ident'>weight_blob</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>learnable_weights</span> {
            <span class='ident'>leaf_cpu_axpy</span>(<span class='kw-2'>&amp;</span><span class='op'>-</span><span class='number'>1f32</span>,
                          <span class='ident'>weight_blob</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>cpu_diff</span>(),
                          <span class='ident'>weight_blob</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>mutable_cpu_data</span>());
        }
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>learnable_weights</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>learnable_weights</span>
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>weights_weight_decay</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>weights_weight_decay</span>
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>weights_lr</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>weights_lr</span>
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the configuration of a network.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// TODO: [DOC] When and why would you use this?</span>
<span class='doccomment'>/// TODO: [DOC] What is the purpose of this configuration type?</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// TODO: [DOC] &lt;Now-What&gt; Examples</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>NetworkConfig</span> {
    <span class='doccomment'>/// Defines the name the network.</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,

    <span class='doccomment'>/// Defines the names of the [input blobs][1].</span>
    <span class='doccomment'>/// [1]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The input blobs are identified by name so they can be referenced as [bottom blobs][2]</span>
    <span class='doccomment'>/// in a [LayerConfig][3].</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ../layer/index.html</span>
    <span class='doccomment'>/// [3]: ../layer/struct.LayerConfig.html</span>
    <span class='ident'>inputs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Defines the [shape][1] of the [input blobs][2].</span>
    <span class='doccomment'>/// [1]: ???</span>
    <span class='doccomment'>/// [2]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The number of input_shapes supplied should match the number of inputs supplied.</span>
    <span class='doccomment'>/// The shape of the input blobs has to be known so that the right connections to the</span>
    <span class='doccomment'>/// upper layers can be set up.</span>
    <span class='ident'>input_shapes</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,

    <span class='doccomment'>/// Defines if the network will force every layer to do [backpropagation][1].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If set to `false`, then the execution of backpropagation is determined automatically</span>
    <span class='doccomment'>/// according to the net structure and learning rates.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: `false`</span>
    <span class='ident'>force_backward</span>: <span class='ident'>bool</span>,

    <span class='doccomment'>/// Defines the [state][1] of the network.</span>
    <span class='doccomment'>/// [1]: ../struct.NetworkState.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Some layers may be included/excluded depending on this state and the states</span>
    <span class='doccomment'>/// specified in the layers&#39; include and exclude fields.</span>
    <span class='kw'>pub</span> <span class='ident'>state</span>: <span class='ident'>NetworkState</span>,

    <span class='doccomment'>/// Defines if the network will print debugging information about results</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: `false`</span>
    <span class='ident'>debug_info</span>: <span class='ident'>bool</span>,

    <span class='doccomment'>/// Defines the layers of the network via [LayerConfig][1]s.</span>
    <span class='doccomment'>/// [1]: ../layer/struct.LayerConfig.html</span>
    <span class='kw'>pub</span> <span class='ident'>layers</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>NetworkConfig</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>NetworkConfig</span> {
        <span class='ident'>NetworkConfig</span> {
            <span class='ident'>name</span>: <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>(),
            <span class='ident'>inputs</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>input_shapes</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),

            <span class='ident'>force_backward</span>: <span class='boolval'>false</span>,
            <span class='ident'>debug_info</span>: <span class='boolval'>false</span>,

            <span class='ident'>layers</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>state</span>: <span class='ident'>NetworkState</span>::<span class='ident'>default</span>(),
        }
    }
}

<span class='kw'>impl</span> <span class='ident'>NetworkConfig</span> {
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>layer</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>layer_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>)
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>input</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>String</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>inputs</span>.<span class='ident'>get</span>(<span class='ident'>input_id</span>)
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>input_shape</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>input_shapes</span>.<span class='ident'>get</span>(<span class='ident'>input_id</span>)
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the state of a network.</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>NetworkState</span> {
    <span class='doccomment'>/// Defines the current mode of the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: Test</span>
    <span class='kw'>pub</span> <span class='ident'>mode</span>: <span class='ident'>NetworkMode</span>,
    <span class='doccomment'>/// TODO: [DOC] what does this do?</span>
    <span class='doccomment'>/// TODO: [DOC] could it be of type usize?</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: 0</span>
    <span class='kw'>pub</span> <span class='ident'>level</span>: <span class='ident'>isize</span>,
    <span class='doccomment'>/// TODO: [DOC] what does this do?</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: vec![]</span>
    <span class='kw'>pub</span> <span class='ident'>stage</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>NetworkState</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>NetworkState</span> {
        <span class='ident'>NetworkState</span> {
            <span class='ident'>mode</span>: <span class='ident'>NetworkMode</span>::<span class='ident'>Test</span>,
            <span class='ident'>level</span>: <span class='number'>0</span>,
            <span class='ident'>stage</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
        }
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Copy</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the possible modes that a network can be in.</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>NetworkMode</span> {
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='ident'>Train</span>,
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='ident'>Test</span>,
}
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>&larrb;</dt>
                    <dd>Move up in search results</dd>
                    <dt>&rarrb;</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "leaf";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script async src="../../search-index.js"></script>
</body>
</html>