<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `src/layer.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>layer.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
</pre><pre class='rust '>
<span class='doccomment'>//! Provides the generics and interfaces for the specific [Layers][layers].</span>
<span class='doccomment'>//! [layers]: ../layers/index.html</span>
<span class='kw'>use</span> <span class='ident'>math</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>phloem</span>::{<span class='ident'>Blob</span>, <span class='ident'>Numeric</span>};
<span class='kw'>use</span> <span class='ident'>shared_memory</span>::{<span class='ident'>ArcLock</span>, <span class='ident'>HeapBlob</span>};
<span class='kw'>use</span> <span class='ident'>layers</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>fmt</span>;

<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>sync</span>::{<span class='ident'>RwLockReadGuard</span>, <span class='ident'>RwLockWriteGuard</span>};

<span class='doccomment'>/// Secures sequential execution as bottom Blob for a forward and as top Blob for a backward</span>
<span class='doccomment'>/// operation.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// Ensures that no layer is reading the HeapBlob, while the current layer is still writing.</span>
<span class='doccomment'>/// The RwLockReadGuard unlocks automatically as soon as the {forward, backward} operation of</span>
<span class='doccomment'>/// the layer is finished and allows for a quick operation transition to the following layer.</span>
<span class='doccomment'>/// Is automatically created by the {forward, backward} method of a [Layer][1] and passed to the</span>
<span class='doccomment'>/// specific [forward_{cpu, gpu}][2] implementation.</span>
<span class='doccomment'>/// [1]: ./trait.ILayer.html#method.forward</span>
<span class='doccomment'>/// [2]: ./trait.ILayer.html#tymethod.forward_cpu</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// ## Example</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// Creates a ReadBlob for seldom scenarios such as testing.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// ```</span>
<span class='doccomment'>/// extern crate phloem;</span>
<span class='doccomment'>/// # extern crate leaf;</span>
<span class='doccomment'>/// use phloem::Blob;</span>
<span class='doccomment'>/// use std::sync::{RwLock, RwLockReadGuard};</span>
<span class='doccomment'>/// # use leaf::layer::ReadBlob;</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// # fn main() {</span>
<span class='doccomment'>/// let lock = RwLock::new(Box::new(Blob::&lt;f32&gt;::of_shape(vec![3])));</span>
<span class='doccomment'>/// let read_blob: ReadBlob = lock.read().unwrap();</span>
<span class='doccomment'>/// # }</span>
<span class='doccomment'>/// ```</span>
<span class='kw'>pub</span> <span class='kw'>type</span> <span class='ident'>ReadBlob</span><span class='op'>&lt;</span><span class='lifetime'>&#39;_</span><span class='op'>&gt;</span> <span class='op'>=</span> <span class='ident'>RwLockReadGuard</span><span class='op'>&lt;</span><span class='lifetime'>&#39;_</span>, <span class='ident'>HeapBlob</span><span class='op'>&gt;</span>;

<span class='doccomment'>/// Secures sequential execution as top Blob for a forward and as bottom Blob for a backward</span>
<span class='doccomment'>/// operation.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// Ensures that no layer is writing to the HeapBlob, while the current layer is still reading it.</span>
<span class='doccomment'>/// The RwLockWriteGuard unlocks automatically as soon as the {forward, backward} operation of</span>
<span class='doccomment'>/// the layer is finished and allows for a quick operation transition to the following layer.</span>
<span class='doccomment'>/// Is automatically created by the {forward, backward} method of a [Layer][1] and passed to the</span>
<span class='doccomment'>/// specific [forward_{cpu, gpu}][2] implementation.</span>
<span class='doccomment'>/// [1]: ./trait.ILayer.html#method.forward</span>
<span class='doccomment'>/// [2]: ./trait.ILayer.html#tymethod.forward_cpu</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// ## Example</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// Creates a ReadBlob for seldom scenarios such as testing.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// ```</span>
<span class='doccomment'>/// extern crate phloem;</span>
<span class='doccomment'>/// # extern crate leaf;</span>
<span class='doccomment'>/// use phloem::Blob;</span>
<span class='doccomment'>/// use std::sync::{RwLock, RwLockWriteGuard};</span>
<span class='doccomment'>/// # use leaf::layer::WriteBlob;</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// # fn main() {</span>
<span class='doccomment'>/// let lock = RwLock::new(Box::new(Blob::&lt;f32&gt;::of_shape(vec![3])));</span>
<span class='doccomment'>/// let read_blob: WriteBlob = lock.write().unwrap();</span>
<span class='doccomment'>/// # }</span>
<span class='doccomment'>/// ```</span>
<span class='kw'>pub</span> <span class='kw'>type</span> <span class='ident'>WriteBlob</span><span class='op'>&lt;</span><span class='lifetime'>&#39;_</span><span class='op'>&gt;</span> <span class='op'>=</span> <span class='ident'>RwLockWriteGuard</span><span class='op'>&lt;</span><span class='lifetime'>&#39;_</span>, <span class='ident'>HeapBlob</span><span class='op'>&gt;</span>;

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>)]</span>
<span class='doccomment'>/// The generic Layer</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>Layer</span> {
    <span class='doccomment'>/// The configuration of the Layer</span>
    <span class='kw'>pub</span> <span class='ident'>config</span>: <span class='ident'>Box</span><span class='op'>&lt;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span>,
    <span class='doccomment'>/// The [implementation][1] of the Layer.</span>
    <span class='doccomment'>/// [1]: ../layers/index.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This is the part that does most of the work ([forward][2]/[backward][3]).</span>
    <span class='doccomment'>/// [2]: ./trait.ILayer.html#method.forward</span>
    <span class='doccomment'>/// [3]: ./trait.ILayer.html#method.backward</span>
    <span class='kw'>pub</span> <span class='ident'>worker</span>: <span class='ident'>Box</span><span class='op'>&lt;</span><span class='ident'>ILayer</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// The vector that indicates whether each top blob contributes to</span>
    <span class='doccomment'>/// the [loss][1] of the network and with which weight.</span>
    <span class='doccomment'>/// [1]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='ident'>loss</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// The vector that stores shared references to the weights in the form of blobs.</span>
    <span class='kw'>pub</span> <span class='ident'>blobs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>,

    <span class='doccomment'>/// Vector indicating whether to compute the diff of each weight blob.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// You can safely ignore false values and always compute gradients</span>
    <span class='doccomment'>/// for all weights, but possibly with wasteful computation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Can be used by some [Layer implementations][1] to optimize performance.</span>
    <span class='doccomment'>/// [1]: ../layers/index.html</span>
    <span class='ident'>weight_propagate_down</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Layer</span> {
    <span class='doccomment'>/// Creates a new Layer from a [LayerConfig][1].</span>
    <span class='doccomment'>/// [1]: ./struct.LayerConfig.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Used during [Network][2] initalization.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ../network/struct.Network.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>from_config</span>(<span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span>) <span class='op'>-&gt;</span> <span class='ident'>Layer</span> {
        <span class='kw'>let</span> <span class='ident'>cl</span> <span class='op'>=</span> <span class='ident'>config</span>.<span class='ident'>clone</span>();
        <span class='kw'>let</span> <span class='ident'>cfg</span> <span class='op'>=</span> <span class='ident'>Box</span>::<span class='op'>&lt;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span>::<span class='ident'>new</span>(<span class='ident'>cl</span>);
        <span class='ident'>Layer</span> {
            <span class='ident'>loss</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>blobs</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),

            <span class='ident'>weight_propagate_down</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),

            <span class='ident'>worker</span>: <span class='ident'>Layer</span>::<span class='ident'>worker_from_config</span>(<span class='kw-2'>&amp;</span><span class='ident'>cfg</span>),
            <span class='ident'>config</span>: <span class='ident'>cfg</span>,
        }
    }

    <span class='doccomment'>/// Helper for [from_config] to match a [LayerType][2] to its [implementation][3].</span>
    <span class='doccomment'>/// [1]: #method.from_config</span>
    <span class='doccomment'>/// [2]: ./enum.LayerType.html</span>
    <span class='doccomment'>/// [3]: ../layers/index.html</span>
    <span class='kw'>fn</span> <span class='ident'>worker_from_config</span>(<span class='ident'>config</span>: <span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span>) <span class='op'>-&gt;</span> <span class='ident'>Box</span><span class='op'>&lt;</span><span class='ident'>ILayer</span><span class='op'>&gt;</span> {
        <span class='kw'>match</span> <span class='ident'>config</span>.<span class='ident'>layer_type</span> {
            <span class='ident'>LayerType</span>::<span class='ident'>Sigmoid</span> <span class='op'>=&gt;</span> <span class='ident'>Box</span>::<span class='ident'>new</span>(<span class='ident'>Sigmoid</span>),
        }
    }

    <span class='doccomment'>/// Sets whether the layer should compute gradients w.r.t. a</span>
    <span class='doccomment'>/// weight at a particular index given by `weight_id`.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// See [`weight_propagate_down`][1]</span>
    <span class='doccomment'>/// ./struct.Layer.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_weight_propagate_down</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>weight_id</span>: <span class='ident'>usize</span>, <span class='ident'>value</span>: <span class='ident'>bool</span>) {
        <span class='kw'>if</span> <span class='self'>self</span>.<span class='ident'>weight_propagate_down</span>.<span class='ident'>len</span>() <span class='op'>&lt;=</span> <span class='ident'>weight_id</span> {
            <span class='self'>self</span>.<span class='ident'>weight_propagate_down</span>.<span class='ident'>resize</span>(<span class='ident'>weight_id</span> <span class='op'>+</span> <span class='number'>1</span>, <span class='boolval'>true</span>);
        }
        <span class='self'>self</span>.<span class='ident'>weight_propagate_down</span>[<span class='ident'>weight_id</span>] <span class='op'>=</span> <span class='ident'>value</span>;

    }

    <span class='doccomment'>/// Returns the [loss weight][1] associated with the weight blob</span>
    <span class='doccomment'>/// with id `weight_id`.</span>
    <span class='doccomment'>/// [1]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>loss</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>weight_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>f32</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>loss</span>.<span class='ident'>get</span>(<span class='ident'>weight_id</span>)
    }
}

<span class='doccomment'>/// A Layer in a [Neural Network][1] that can handle forward and backward of a computation step.</span>
<span class='doccomment'>/// [1]: ../network/index.html</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>ILayer</span> {
    <span class='doccomment'>/// Compute the [feedforward][1] layer output.</span>
    <span class='doccomment'>/// Uses the CPU.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='kw'>fn</span> <span class='ident'>forward_cpu</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>bottom</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ReadBlob</span>], <span class='ident'>top</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>WriteBlob</span><span class='op'>&gt;</span>);
    <span class='doccomment'>/// Compute the gradients for the bottom blobs</span>
    <span class='doccomment'>/// if the corresponding value of propagate_down is true.</span>
    <span class='doccomment'>/// Uses the CPU.</span>
    <span class='kw'>fn</span> <span class='ident'>backward_cpu</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>top</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>HeapBlob</span>], <span class='ident'>propagate_down</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>bool</span>], <span class='ident'>bottom</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;</span>);

    <span class='doccomment'>/// Compute the [feedforward][1] layer output using the currently set computation method.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Aquires read locks for the bottom blobs ([ReadBlob][2])</span>
    <span class='doccomment'>/// and write locks for the top blobs ([WriteBlob][3]) to ensure sequential computation,</span>
    <span class='doccomment'>/// and then passes them to computation method specific function ([forward_cpu][4]).</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ./type.ReadBlob.html</span>
    <span class='doccomment'>/// [3]: ./type.WriteBlob.html</span>
    <span class='doccomment'>/// [3]: #method.forward_cpu</span>
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>map_clone</span>)]</span>
    <span class='kw'>fn</span> <span class='ident'>forward</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>bottom</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;</span>], <span class='ident'>top</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>HeapBlob</span><span class='op'>&gt;&gt;</span>) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='comment'>// Lock();</span>
        <span class='comment'>// Reshape(bottom, top); // Reshape the layer to fit top &amp; bottom blob</span>
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>loss</span> <span class='op'>=</span> <span class='number'>0f32</span>;

        <span class='kw'>let</span> <span class='ident'>btm</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span>_<span class='op'>&gt;</span> <span class='op'>=</span> <span class='ident'>bottom</span>.<span class='ident'>iter</span>().<span class='ident'>map</span>(<span class='op'>|</span><span class='ident'>b</span><span class='op'>|</span> <span class='ident'>b</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>()).<span class='ident'>collect</span>();
        <span class='comment'>// let tp: Vec&lt;_&gt; = top.iter().map(|b| b.write().unwrap()).collect();</span>
        <span class='kw'>let</span> <span class='ident'>tp_ref</span> <span class='op'>=</span> <span class='ident'>top</span>.<span class='ident'>iter</span>().<span class='ident'>cloned</span>().<span class='ident'>collect</span>::<span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span>_<span class='op'>&gt;&gt;</span>();
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>tp</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>tp_ref</span>.<span class='ident'>iter</span>().<span class='ident'>map</span>(<span class='op'>|</span><span class='ident'>b</span><span class='op'>|</span> <span class='ident'>b</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>()).<span class='ident'>collect</span>::<span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span>_<span class='op'>&gt;&gt;</span>();
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>tpo</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>tp</span>.<span class='ident'>iter_mut</span>().<span class='ident'>map</span>(<span class='op'>|</span><span class='ident'>a</span><span class='op'>|</span> <span class='ident'>a</span>).<span class='ident'>collect</span>::<span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span>_<span class='op'>&gt;&gt;</span>();
        <span class='self'>self</span>.<span class='ident'>forward_cpu</span>(<span class='kw-2'>&amp;</span><span class='ident'>btm</span>, <span class='ident'>tpo</span>);
        <span class='comment'>// self.forward_cpu(bottom, top);</span>

        <span class='kw'>for</span> (<span class='ident'>top_id</span>, <span class='ident'>top_layer</span>) <span class='kw'>in</span> <span class='ident'>top</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='comment'>// if (!this-&gt;loss(top_id)) { continue; } // Caffe</span>
            <span class='comment'>// if !self.loss(top_id) { continue; }</span>

            <span class='kw'>let</span> <span class='ident'>top_blob</span> <span class='op'>=</span> <span class='ident'>top_layer</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>();

            <span class='kw'>let</span> <span class='ident'>data</span> <span class='op'>=</span> <span class='ident'>top_blob</span>.<span class='ident'>cpu_data</span>();
            <span class='kw'>let</span> <span class='ident'>loss_weights</span> <span class='op'>=</span> <span class='ident'>top_blob</span>.<span class='ident'>cpu_diff</span>();

            <span class='ident'>loss</span> <span class='op'>+=</span> <span class='ident'>leaf_cpu_dot</span>(<span class='ident'>data</span>, <span class='ident'>loss_weights</span>);
        }

        <span class='comment'>// Unlock();</span>

        <span class='ident'>loss</span>
    }

    <span class='doccomment'>/// Return whether &quot;anonymous&quot; top blobs are created automatically for the layer.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If this method returns true, Network::init will create enough &quot;anonymous&quot; top</span>
    <span class='doccomment'>/// blobs to fulfill the requirement specified by exact_num_top_blobs() or</span>
    <span class='doccomment'>/// min_top_blobs().</span>
    <span class='kw'>fn</span> <span class='ident'>auto_top_blobs</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>bool</span> {
        <span class='boolval'>false</span>
    }
    <span class='doccomment'>/// Returns the minimum number of top blobs required by the layer,</span>
    <span class='doccomment'>/// or 0 if no minimum number is required.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This method should be overridden to return a positive value if your</span>
    <span class='doccomment'>/// layer expects some minimum number of top blobs.</span>
    <span class='kw'>fn</span> <span class='ident'>min_top_blobs</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='number'>0</span>
    }
    <span class='doccomment'>/// Returns the exact number of top blobs required by the layer,</span>
    <span class='doccomment'>/// or 0 if no exact number is required.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This method should be overridden to return a positive value if your</span>
    <span class='doccomment'>/// layer expects some exact number of top blobs.</span>
    <span class='kw'>fn</span> <span class='ident'>exact_num_top_blobs</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='number'>0</span>
    }
    <span class='doccomment'>/// Returns the exact number of bottom blobs required by the layer,</span>
    <span class='doccomment'>/// or 0 if no exact number is required.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This method should be overridden to return a positive value if your</span>
    <span class='doccomment'>/// layer expects some exact number of bottom blobs.</span>
    <span class='kw'>fn</span> <span class='ident'>exact_num_bottom_blobs</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='number'>0</span>
    }
    <span class='doccomment'>/// Return whether to allow force_backward for a given bottom blob index.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If AllowForceBackward(i) == false, we will ignore the force_backward</span>
    <span class='doccomment'>/// setting and backpropagate to blob i only if it needs gradient information</span>
    <span class='doccomment'>/// (as is done when force_backward == false).</span>
    <span class='kw'>fn</span> <span class='ident'>allow_force_backward</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>bottom_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='ident'>bool</span> {
        <span class='boolval'>true</span>
    }
}

<span class='kw'>impl</span> <span class='ident'>fmt</span>::<span class='ident'>Debug</span> <span class='kw'>for</span> <span class='ident'>ILayer</span> {
    <span class='kw'>fn</span> <span class='ident'>fmt</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>f</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>fmt</span>::<span class='ident'>Formatter</span>) <span class='op'>-&gt;</span> <span class='ident'>fmt</span>::<span class='prelude-ty'>Result</span> {
        <span class='macro'>write</span><span class='macro'>!</span>(<span class='ident'>f</span>, <span class='string'>&quot;({}, {})&quot;</span>, <span class='string'>&quot;foo&quot;</span>, <span class='string'>&quot;bar&quot;</span>)
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Layer Configuration Struct</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>LayerConfig</span> {
    <span class='doccomment'>/// The name of the Layer</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,

    <span class='doccomment'>/// The type of the Layer</span>
    <span class='ident'>layer_type</span>: <span class='ident'>LayerType</span>,

    <span class='doccomment'>/// The name for each top Blob</span>
    <span class='ident'>tops</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// The name for each bottom Blob</span>
    <span class='ident'>bottoms</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Specifies training configuration for each weight blob.</span>
    <span class='ident'>params</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>WeightConfig</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Specifies on which bottoms the backpropagation should be skipped.</span>
    <span class='doccomment'>/// The size must be either 0 or equal to the number of bottoms.</span>
    <span class='kw'>pub</span> <span class='ident'>propagate_down</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>bool</span><span class='op'>&gt;</span>,
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Copy</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// The Layer Types</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>LayerType</span> {
    <span class='doccomment'>/// Sigmoid Layer</span>
    <span class='ident'>Sigmoid</span>,
}

<span class='kw'>impl</span> <span class='ident'>LayerConfig</span> {
    <span class='doccomment'>/// Creates a new LayerConfig</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>new</span>(<span class='ident'>name</span>: <span class='ident'>String</span>, <span class='ident'>layer_type</span>: <span class='ident'>LayerType</span>) <span class='op'>-&gt;</span> <span class='ident'>LayerConfig</span> {
        <span class='ident'>LayerConfig</span> {
            <span class='ident'>name</span>: <span class='ident'>name</span>,
            <span class='ident'>layer_type</span>: <span class='ident'>layer_type</span>,

            <span class='ident'>tops</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>bottoms</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),

            <span class='ident'>params</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>propagate_down</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
        }
    }

    <span class='doccomment'>/// Returns the Name of the requested top Blob</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>top</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>top_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>String</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>tops</span>.<span class='ident'>get</span>(<span class='ident'>top_id</span>)
    }

    <span class='doccomment'>/// Returns the number of top Blobs</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>tops_len</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='self'>self</span>.<span class='ident'>tops</span>.<span class='ident'>len</span>()
    }

    <span class='doccomment'>/// Returns the Name of the requested bottom Blob</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>bottom</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>bottom_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>String</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>bottoms</span>.<span class='ident'>get</span>(<span class='ident'>bottom_id</span>)
    }

    <span class='doccomment'>/// Returns the number of bottom Blobs</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>bottoms_len</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='self'>self</span>.<span class='ident'>bottoms</span>.<span class='ident'>len</span>()
    }

    <span class='doccomment'>/// Returns the requested WeightConfig</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>param</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>param_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>WeightConfig</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>params</span>.<span class='ident'>get</span>(<span class='ident'>param_id</span>)
    }

    <span class='doccomment'>/// Returns the number of params</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>params_len</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>usize</span> {
        <span class='self'>self</span>.<span class='ident'>params</span>.<span class='ident'>len</span>()
    }

    <span class='doccomment'>/// Checks if propagate down length is sane</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>check_propagate_down_len</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>bool</span> {
        <span class='self'>self</span>.<span class='ident'>propagate_down</span>.<span class='ident'>is_empty</span>() <span class='op'>||</span> <span class='self'>self</span>.<span class='ident'>propagate_down</span>.<span class='ident'>len</span>() <span class='op'>==</span> <span class='self'>self</span>.<span class='ident'>bottoms</span>.<span class='ident'>len</span>()
    }
}


<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Specifies training configuration for a weight blob.</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>WeightConfig</span> {
    <span class='doccomment'>/// The name of the weight blob -- useful for sharing weights among</span>
    <span class='doccomment'>/// layers, but never required otherwise. To share a weight between two</span>
    <span class='doccomment'>/// layers, give it a (non-empty) name.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: &quot;&quot;</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,
    <span class='doccomment'>/// Whether to require shared weights to have the same shape, or just the same</span>
    <span class='doccomment'>/// count</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: DimCheckMode::Strict</span>
    <span class='kw'>pub</span> <span class='ident'>share_mode</span>: <span class='ident'>DimCheckMode</span>,

    <span class='doccomment'>/// The multiplier on the global learning rate for this parameter.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: 1.0f32</span>
    <span class='kw'>pub</span> <span class='ident'>lr_mult</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// The multiplier on the global weight decay for this parameter.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: 1.0f32</span>
    <span class='kw'>pub</span> <span class='ident'>decay_mult</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>WeightConfig</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>WeightConfig</span> {
        <span class='ident'>WeightConfig</span> {
            <span class='ident'>name</span>: <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>(),
            <span class='ident'>share_mode</span>: <span class='ident'>DimCheckMode</span>::<span class='ident'>Strict</span>,
            <span class='ident'>lr_mult</span>: <span class='prelude-val'>None</span>,
            <span class='ident'>decay_mult</span>: <span class='prelude-val'>None</span>,
        }
    }
}

<span class='kw'>impl</span> <span class='ident'>WeightConfig</span> {
    <span class='doccomment'>/// Checks dimensions of two blobs according to the `share_mode`.</span>
    <span class='doccomment'>/// Returns an error if there is a count/shape mismatch.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>check_dimensions</span><span class='op'>&lt;</span><span class='ident'>T</span>: <span class='ident'>Numeric</span><span class='op'>&gt;</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>,
                                        <span class='ident'>blob_one</span>: <span class='kw-2'>&amp;</span><span class='ident'>Blob</span><span class='op'>&lt;</span><span class='ident'>T</span><span class='op'>&gt;</span>,
                                        <span class='ident'>blob_two</span>: <span class='kw-2'>&amp;</span><span class='ident'>Blob</span><span class='op'>&lt;</span><span class='ident'>T</span><span class='op'>&gt;</span>,
                                        <span class='ident'>param_name</span>: <span class='ident'>String</span>,
                                        <span class='ident'>owner_name</span>: <span class='ident'>String</span>,
                                        <span class='ident'>layer_name</span>: <span class='ident'>String</span>)
                                        <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), <span class='ident'>String</span><span class='op'>&gt;</span> {
        <span class='kw'>match</span> <span class='self'>self</span>.<span class='ident'>share_mode</span> {
            <span class='comment'>// Permissive dimension checking -- only check counts are the same.</span>
            <span class='ident'>DimCheckMode</span>::<span class='ident'>Permissive</span> <span class='op'>=&gt;</span> {
                <span class='kw'>if</span> <span class='ident'>blob_one</span>.<span class='ident'>capacity</span>() <span class='op'>!=</span> <span class='ident'>blob_two</span>.<span class='ident'>capacity</span>() {
                    <span class='kw'>return</span> <span class='prelude-val'>Err</span>(<span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;Cannot share weight &#39;{}&#39; owned by layer &#39;{}&#39; with layer &#39;{}&#39;;
                                count mismatch.
                                Owner layer weight shape is {};
                                Sharing layer weight shape is {}&quot;</span>,
                                       <span class='ident'>param_name</span>,
                                       <span class='ident'>owner_name</span>,
                                       <span class='ident'>layer_name</span>,
                                       <span class='ident'>blob_two</span>.<span class='ident'>shape_string</span>(),
                                       <span class='ident'>blob_one</span>.<span class='ident'>shape_string</span>()));
                }
            }
            <span class='comment'>// Strict dimension checking -- all dims must be the same.</span>
            <span class='ident'>DimCheckMode</span>::<span class='ident'>Strict</span> <span class='op'>=&gt;</span> {
                <span class='kw'>if</span> <span class='ident'>blob_one</span>.<span class='ident'>shape</span>() <span class='op'>!=</span> <span class='ident'>blob_two</span>.<span class='ident'>shape</span>() {
                    <span class='kw'>return</span> <span class='prelude-val'>Err</span>(<span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;Cannot share weight &#39;{}&#39; owned by layer &#39;{}&#39; with layer &#39;{}&#39;;
                                shape mismatch.
                                Owner layer weight shape is {};
                                Sharing layer expects weight shape {}&quot;</span>,
                                       <span class='ident'>param_name</span>,
                                       <span class='ident'>owner_name</span>,
                                       <span class='ident'>layer_name</span>,
                                       <span class='ident'>blob_two</span>.<span class='ident'>shape_string</span>(),
                                       <span class='ident'>blob_one</span>.<span class='ident'>shape_string</span>()));
                }
            }
        }
        <span class='prelude-val'>Ok</span>(())
    }

    <span class='doccomment'>/// The multiplier on the global learning rate for this weight blob.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>lr_mult</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='kw'>match</span> <span class='self'>self</span>.<span class='ident'>lr_mult</span> {
            <span class='prelude-val'>Some</span>(<span class='ident'>val</span>) <span class='op'>=&gt;</span> <span class='ident'>val</span>,
            <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> <span class='number'>1.0f32</span>,
        }
    }

    <span class='doccomment'>/// The multiplier on the global weight decay for this weight blob.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>decay_mult</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='kw'>match</span> <span class='self'>self</span>.<span class='ident'>decay_mult</span> {
            <span class='prelude-val'>Some</span>(<span class='ident'>val</span>) <span class='op'>=&gt;</span> <span class='ident'>val</span>,
            <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> <span class='number'>1.0f32</span>,
        }
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Copy</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Enum for specifing the shared weights behaviour</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>DimCheckMode</span> {
    <span class='doccomment'>/// Strict requires that shapes match.</span>
    <span class='ident'>Strict</span>,
    <span class='doccomment'>/// Permissive requires only the count of weights to match.</span>
    <span class='ident'>Permissive</span>,
}
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>&larrb;</dt>
                    <dd>Move up in search results</dd>
                    <dt>&rarrb;</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "leaf";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script async src="../../search-index.js"></script>
</body>
</html>